{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id                     breed\n",
      "0      000bec180eb18c7604dcecc8fe0dba07               boston_bull\n",
      "1      001513dfcb2ffafc82cccf4d8bbaba97                     dingo\n",
      "2      001cdf01b096e06d78e9e5112d419397                  pekinese\n",
      "3      00214f311d5d2247d5dfe4fe24b2303d                  bluetick\n",
      "4      0021f9ceb3235effd7fcde7f7538ed62          golden_retriever\n",
      "...                                 ...                       ...\n",
      "10217  ffd25009d635cfd16e793503ac5edef0                    borzoi\n",
      "10218  ffd3f636f7f379c51ba3648a9ff8254f            dandie_dinmont\n",
      "10219  ffe2ca6c940cddfee68fa3cc6c63213f                  airedale\n",
      "10220  ffe5f6d8e2bff356e9482a80a6e29aac        miniature_pinscher\n",
      "10221  fff43b07992508bc822f33d8ffd902ae  chesapeake_bay_retriever\n",
      "\n",
      "[10222 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#TEST_IMAGE_DIR = \"./dog-breed-identification/test/\"\n",
    "TRAIN_IMAGE_DIR = \"./dog-breed-identification/train/\"\n",
    "LABELS = './dog-breed-identification/labels.csv'\n",
    "\n",
    "data = pd.read_csv(LABELS)\n",
    "class_names = data['breed'].unique()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data from the csv only contans the ids of the photos and not the photos itself\n",
    "#Iterate through the data and check whether the photo is in test or train\n",
    "#Then fetch it and store it in its proper variable\n",
    "import tensorflow as tf\n",
    "from os.path import join\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def load_and_preprocess_images(image_dir, image_ids, target_size=(224, 224)):\n",
    "    image_data = []\n",
    "    for img_id in image_ids:\n",
    "        img_path = join(image_dir, img_id + \".jpg\")\n",
    "        img = load_img(img_path, target_size=target_size)\n",
    "        img_array = img_to_array(img)\n",
    "        preprocessed_img = preprocess_input(img_array) #I dont flatten the image here\n",
    "        image_data.append(preprocessed_img)\n",
    "    return np.array(image_data)\n",
    "\n",
    "# Get image IDs for train and test\n",
    "train_image_ids = data['id'].values\n",
    "labels = data['breed'].values\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "train_image_ids, unfinished_test_image_ids, train_labels, unfinished_test_labels = train_test_split(train_image_ids, encoded_labels, test_size=0.3, random_state=42, stratify=encoded_labels)\n",
    "\n",
    "test_image_ids, val_image_ids, test_labels, val_labels = train_test_split(unfinished_test_image_ids, unfinished_test_labels, test_size=0.5, random_state=42, stratify=unfinished_test_labels)\n",
    "\n",
    "train_images = load_and_preprocess_images(TRAIN_IMAGE_DIR, train_image_ids)\n",
    "test_images = load_and_preprocess_images(TRAIN_IMAGE_DIR, test_image_ids)\n",
    "val_images = load_and_preprocess_images(TRAIN_IMAGE_DIR, val_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 250.0\n",
    "test_images = test_images / 250.0\n",
    "val_images = val_images / 250.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2\n",
    ")\n",
    "\n",
    "datagen.fit(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in base_model.layers[:-4]:  # Freeze all layers except the last 4\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in base_model.layers[-4:]:  # Set the last 4 layers to trainable\n",
    "    layer.trainable = True\n",
    "\n",
    "# Build the custom classification head\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(x)  # Increase nodes from 512 to 1024\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)  # Add an additional layer\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "predictions = tf.keras.layers.Dense(120, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=1e-5)\n",
    "model.compile(optimizer=optimizer, #Not using adam anymore\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#from keras.callbacks import Callback\n",
    "\n",
    "filepath = './models/best_model.epoch{epoch:02d}-loss{val_loss:.2f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, \n",
    "                             monitor='val_loss',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 6.1450 - accuracy: 0.0066\n",
      "Epoch 1: val_loss improved from inf to 4.84471, saving model to ./models\\best_model.epoch01-loss4.84.hdf5\n",
      "89/89 [==============================] - 60s 576ms/step - loss: 6.1450 - accuracy: 0.0066 - val_loss: 4.8447 - val_accuracy: 0.0098\n",
      "Epoch 2/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.9848 - accuracy: 0.0098\n",
      "Epoch 2: val_loss did not improve from 4.84471\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 5.9848 - accuracy: 0.0098 - val_loss: 4.9016 - val_accuracy: 0.0104\n",
      "Epoch 3/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.9845 - accuracy: 0.0090\n",
      "Epoch 3: val_loss did not improve from 4.84471\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 5.9845 - accuracy: 0.0090 - val_loss: 4.9176 - val_accuracy: 0.0111\n",
      "Epoch 4/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.9312 - accuracy: 0.0089\n",
      "Epoch 4: val_loss did not improve from 4.84471\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 5.9312 - accuracy: 0.0089 - val_loss: 4.8983 - val_accuracy: 0.0085\n",
      "Epoch 5/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.8600 - accuracy: 0.0117\n",
      "Epoch 5: val_loss did not improve from 4.84471\n",
      "89/89 [==============================] - 45s 502ms/step - loss: 5.8600 - accuracy: 0.0117 - val_loss: 4.8547 - val_accuracy: 0.0137\n",
      "Epoch 6/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.8622 - accuracy: 0.0112\n",
      "Epoch 6: val_loss improved from 4.84471 to 4.80668, saving model to ./models\\best_model.epoch06-loss4.81.hdf5\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 5.8622 - accuracy: 0.0112 - val_loss: 4.8067 - val_accuracy: 0.0163\n",
      "Epoch 7/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.7667 - accuracy: 0.0113\n",
      "Epoch 7: val_loss improved from 4.80668 to 4.74152, saving model to ./models\\best_model.epoch07-loss4.74.hdf5\n",
      "89/89 [==============================] - 45s 507ms/step - loss: 5.7667 - accuracy: 0.0113 - val_loss: 4.7415 - val_accuracy: 0.0202\n",
      "Epoch 8/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.7283 - accuracy: 0.0123\n",
      "Epoch 8: val_loss improved from 4.74152 to 4.68140, saving model to ./models\\best_model.epoch08-loss4.68.hdf5\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 5.7283 - accuracy: 0.0123 - val_loss: 4.6814 - val_accuracy: 0.0326\n",
      "Epoch 9/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.6862 - accuracy: 0.0143\n",
      "Epoch 9: val_loss improved from 4.68140 to 4.63857, saving model to ./models\\best_model.epoch09-loss4.64.hdf5\n",
      "89/89 [==============================] - 45s 504ms/step - loss: 5.6862 - accuracy: 0.0143 - val_loss: 4.6386 - val_accuracy: 0.0352\n",
      "Epoch 10/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.6544 - accuracy: 0.0147\n",
      "Epoch 10: val_loss improved from 4.63857 to 4.60532, saving model to ./models\\best_model.epoch10-loss4.61.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 5.6544 - accuracy: 0.0147 - val_loss: 4.6053 - val_accuracy: 0.0443\n",
      "Epoch 11/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.5913 - accuracy: 0.0164\n",
      "Epoch 11: val_loss improved from 4.60532 to 4.56894, saving model to ./models\\best_model.epoch11-loss4.57.hdf5\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 5.5913 - accuracy: 0.0164 - val_loss: 4.5689 - val_accuracy: 0.0476\n",
      "Epoch 12/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.5150 - accuracy: 0.0154\n",
      "Epoch 12: val_loss improved from 4.56894 to 4.47248, saving model to ./models\\best_model.epoch12-loss4.47.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 5.5150 - accuracy: 0.0154 - val_loss: 4.4725 - val_accuracy: 0.0567\n",
      "Epoch 13/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.5304 - accuracy: 0.0181\n",
      "Epoch 13: val_loss improved from 4.47248 to 4.42555, saving model to ./models\\best_model.epoch13-loss4.43.hdf5\n",
      "89/89 [==============================] - 45s 502ms/step - loss: 5.5304 - accuracy: 0.0181 - val_loss: 4.4255 - val_accuracy: 0.0626\n",
      "Epoch 14/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.4644 - accuracy: 0.0218\n",
      "Epoch 14: val_loss improved from 4.42555 to 4.40453, saving model to ./models\\best_model.epoch14-loss4.40.hdf5\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 5.4644 - accuracy: 0.0218 - val_loss: 4.4045 - val_accuracy: 0.0698\n",
      "Epoch 15/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.4162 - accuracy: 0.0209\n",
      "Epoch 15: val_loss improved from 4.40453 to 4.37851, saving model to ./models\\best_model.epoch15-loss4.38.hdf5\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 5.4162 - accuracy: 0.0209 - val_loss: 4.3785 - val_accuracy: 0.0717\n",
      "Epoch 16/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.3595 - accuracy: 0.0257\n",
      "Epoch 16: val_loss improved from 4.37851 to 4.31616, saving model to ./models\\best_model.epoch16-loss4.32.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 5.3595 - accuracy: 0.0257 - val_loss: 4.3162 - val_accuracy: 0.0776\n",
      "Epoch 17/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.3163 - accuracy: 0.0276\n",
      "Epoch 17: val_loss improved from 4.31616 to 4.26924, saving model to ./models\\best_model.epoch17-loss4.27.hdf5\n",
      "89/89 [==============================] - 45s 502ms/step - loss: 5.3163 - accuracy: 0.0276 - val_loss: 4.2692 - val_accuracy: 0.0874\n",
      "Epoch 18/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.2807 - accuracy: 0.0254\n",
      "Epoch 18: val_loss improved from 4.26924 to 4.22931, saving model to ./models\\best_model.epoch18-loss4.23.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 5.2807 - accuracy: 0.0254 - val_loss: 4.2293 - val_accuracy: 0.0926\n",
      "Epoch 19/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.2055 - accuracy: 0.0318\n",
      "Epoch 19: val_loss improved from 4.22931 to 4.20579, saving model to ./models\\best_model.epoch19-loss4.21.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 5.2055 - accuracy: 0.0318 - val_loss: 4.2058 - val_accuracy: 0.0997\n",
      "Epoch 20/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.1975 - accuracy: 0.0314\n",
      "Epoch 20: val_loss improved from 4.20579 to 4.15456, saving model to ./models\\best_model.epoch20-loss4.15.hdf5\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 5.1975 - accuracy: 0.0314 - val_loss: 4.1546 - val_accuracy: 0.1010\n",
      "Epoch 21/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.1716 - accuracy: 0.0329\n",
      "Epoch 21: val_loss improved from 4.15456 to 4.13265, saving model to ./models\\best_model.epoch21-loss4.13.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 5.1716 - accuracy: 0.0329 - val_loss: 4.1326 - val_accuracy: 0.0978\n",
      "Epoch 22/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.1028 - accuracy: 0.0366\n",
      "Epoch 22: val_loss improved from 4.13265 to 4.08287, saving model to ./models\\best_model.epoch22-loss4.08.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 5.1028 - accuracy: 0.0366 - val_loss: 4.0829 - val_accuracy: 0.1147\n",
      "Epoch 23/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.0350 - accuracy: 0.0387\n",
      "Epoch 23: val_loss improved from 4.08287 to 4.03730, saving model to ./models\\best_model.epoch23-loss4.04.hdf5\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 5.0350 - accuracy: 0.0387 - val_loss: 4.0373 - val_accuracy: 0.1180\n",
      "Epoch 24/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.0483 - accuracy: 0.0376\n",
      "Epoch 24: val_loss improved from 4.03730 to 4.00030, saving model to ./models\\best_model.epoch24-loss4.00.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 5.0483 - accuracy: 0.0376 - val_loss: 4.0003 - val_accuracy: 0.1160\n",
      "Epoch 25/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.0130 - accuracy: 0.0406\n",
      "Epoch 25: val_loss improved from 4.00030 to 3.95861, saving model to ./models\\best_model.epoch25-loss3.96.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 5.0130 - accuracy: 0.0406 - val_loss: 3.9586 - val_accuracy: 0.1310\n",
      "Epoch 26/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.9433 - accuracy: 0.0421\n",
      "Epoch 26: val_loss improved from 3.95861 to 3.92458, saving model to ./models\\best_model.epoch26-loss3.92.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 4.9433 - accuracy: 0.0421 - val_loss: 3.9246 - val_accuracy: 0.1389\n",
      "Epoch 27/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.9256 - accuracy: 0.0455\n",
      "Epoch 27: val_loss improved from 3.92458 to 3.92011, saving model to ./models\\best_model.epoch27-loss3.92.hdf5\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 4.9256 - accuracy: 0.0455 - val_loss: 3.9201 - val_accuracy: 0.1258\n",
      "Epoch 28/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.8652 - accuracy: 0.0468\n",
      "Epoch 28: val_loss improved from 3.92011 to 3.86971, saving model to ./models\\best_model.epoch28-loss3.87.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 4.8652 - accuracy: 0.0468 - val_loss: 3.8697 - val_accuracy: 0.1349\n",
      "Epoch 29/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.8688 - accuracy: 0.0495\n",
      "Epoch 29: val_loss improved from 3.86971 to 3.81693, saving model to ./models\\best_model.epoch29-loss3.82.hdf5\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 4.8688 - accuracy: 0.0495 - val_loss: 3.8169 - val_accuracy: 0.1402\n",
      "Epoch 30/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.8221 - accuracy: 0.0472\n",
      "Epoch 30: val_loss improved from 3.81693 to 3.79767, saving model to ./models\\best_model.epoch30-loss3.80.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 4.8221 - accuracy: 0.0472 - val_loss: 3.7977 - val_accuracy: 0.1519\n",
      "Epoch 31/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.7583 - accuracy: 0.0543\n",
      "Epoch 31: val_loss improved from 3.79767 to 3.76094, saving model to ./models\\best_model.epoch31-loss3.76.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 4.7583 - accuracy: 0.0543 - val_loss: 3.7609 - val_accuracy: 0.1558\n",
      "Epoch 32/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.7287 - accuracy: 0.0540\n",
      "Epoch 32: val_loss improved from 3.76094 to 3.73887, saving model to ./models\\best_model.epoch32-loss3.74.hdf5\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 4.7287 - accuracy: 0.0540 - val_loss: 3.7389 - val_accuracy: 0.1662\n",
      "Epoch 33/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.7308 - accuracy: 0.0554\n",
      "Epoch 33: val_loss improved from 3.73887 to 3.70316, saving model to ./models\\best_model.epoch33-loss3.70.hdf5\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 4.7308 - accuracy: 0.0554 - val_loss: 3.7032 - val_accuracy: 0.1591\n",
      "Epoch 34/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.6702 - accuracy: 0.0588\n",
      "Epoch 34: val_loss improved from 3.70316 to 3.66861, saving model to ./models\\best_model.epoch34-loss3.67.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 4.6702 - accuracy: 0.0588 - val_loss: 3.6686 - val_accuracy: 0.1701\n",
      "Epoch 35/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.6380 - accuracy: 0.0615\n",
      "Epoch 35: val_loss improved from 3.66861 to 3.66135, saving model to ./models\\best_model.epoch35-loss3.66.hdf5\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 4.6380 - accuracy: 0.0615 - val_loss: 3.6613 - val_accuracy: 0.1734\n",
      "Epoch 36/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.6094 - accuracy: 0.0628\n",
      "Epoch 36: val_loss improved from 3.66135 to 3.61046, saving model to ./models\\best_model.epoch36-loss3.61.hdf5\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 4.6094 - accuracy: 0.0628 - val_loss: 3.6105 - val_accuracy: 0.1871\n",
      "Epoch 37/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.5923 - accuracy: 0.0654\n",
      "Epoch 37: val_loss did not improve from 3.61046\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 4.5923 - accuracy: 0.0654 - val_loss: 3.6162 - val_accuracy: 0.1832\n",
      "Epoch 38/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.5041 - accuracy: 0.0735\n",
      "Epoch 38: val_loss improved from 3.61046 to 3.57249, saving model to ./models\\best_model.epoch38-loss3.57.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 4.5041 - accuracy: 0.0735 - val_loss: 3.5725 - val_accuracy: 0.1923\n",
      "Epoch 39/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.5071 - accuracy: 0.0722\n",
      "Epoch 39: val_loss improved from 3.57249 to 3.54601, saving model to ./models\\best_model.epoch39-loss3.55.hdf5\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 4.5071 - accuracy: 0.0722 - val_loss: 3.5460 - val_accuracy: 0.2053\n",
      "Epoch 40/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.4697 - accuracy: 0.0719\n",
      "Epoch 40: val_loss improved from 3.54601 to 3.51351, saving model to ./models\\best_model.epoch40-loss3.51.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 4.4697 - accuracy: 0.0719 - val_loss: 3.5135 - val_accuracy: 0.2112\n",
      "Epoch 41/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.4345 - accuracy: 0.0807\n",
      "Epoch 41: val_loss improved from 3.51351 to 3.46886, saving model to ./models\\best_model.epoch41-loss3.47.hdf5\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 4.4345 - accuracy: 0.0807 - val_loss: 3.4689 - val_accuracy: 0.2190\n",
      "Epoch 42/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.4235 - accuracy: 0.0804\n",
      "Epoch 42: val_loss improved from 3.46886 to 3.44277, saving model to ./models\\best_model.epoch42-loss3.44.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 4.4235 - accuracy: 0.0804 - val_loss: 3.4428 - val_accuracy: 0.2210\n",
      "Epoch 43/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.3980 - accuracy: 0.0792\n",
      "Epoch 43: val_loss improved from 3.44277 to 3.43328, saving model to ./models\\best_model.epoch43-loss3.43.hdf5\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 4.3980 - accuracy: 0.0792 - val_loss: 3.4333 - val_accuracy: 0.2327\n",
      "Epoch 44/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.3425 - accuracy: 0.0879\n",
      "Epoch 44: val_loss improved from 3.43328 to 3.39359, saving model to ./models\\best_model.epoch44-loss3.39.hdf5\n",
      "89/89 [==============================] - 45s 504ms/step - loss: 4.3425 - accuracy: 0.0879 - val_loss: 3.3936 - val_accuracy: 0.2445\n",
      "Epoch 45/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.3370 - accuracy: 0.0848\n",
      "Epoch 45: val_loss improved from 3.39359 to 3.32073, saving model to ./models\\best_model.epoch45-loss3.32.hdf5\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 4.3370 - accuracy: 0.0848 - val_loss: 3.3207 - val_accuracy: 0.2419\n",
      "Epoch 46/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.3071 - accuracy: 0.0868\n",
      "Epoch 46: val_loss improved from 3.32073 to 3.30576, saving model to ./models\\best_model.epoch46-loss3.31.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 4.3071 - accuracy: 0.0868 - val_loss: 3.3058 - val_accuracy: 0.2621\n",
      "Epoch 47/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.2799 - accuracy: 0.0919\n",
      "Epoch 47: val_loss did not improve from 3.30576\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 4.2799 - accuracy: 0.0919 - val_loss: 3.3297 - val_accuracy: 0.2634\n",
      "Epoch 48/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.2303 - accuracy: 0.0978\n",
      "Epoch 48: val_loss improved from 3.30576 to 3.29315, saving model to ./models\\best_model.epoch48-loss3.29.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 4.2303 - accuracy: 0.0978 - val_loss: 3.2932 - val_accuracy: 0.2490\n",
      "Epoch 49/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.2341 - accuracy: 0.0943\n",
      "Epoch 49: val_loss improved from 3.29315 to 3.29119, saving model to ./models\\best_model.epoch49-loss3.29.hdf5\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 4.2341 - accuracy: 0.0943 - val_loss: 3.2912 - val_accuracy: 0.2718\n",
      "Epoch 50/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.1951 - accuracy: 0.0965\n",
      "Epoch 50: val_loss improved from 3.29119 to 3.21725, saving model to ./models\\best_model.epoch50-loss3.22.hdf5\n",
      "89/89 [==============================] - 44s 500ms/step - loss: 4.1951 - accuracy: 0.0965 - val_loss: 3.2172 - val_accuracy: 0.2712\n",
      "Epoch 51/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.1585 - accuracy: 0.1012\n",
      "Epoch 51: val_loss did not improve from 3.21725\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 4.1585 - accuracy: 0.1012 - val_loss: 3.2328 - val_accuracy: 0.2562\n",
      "Epoch 52/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.1314 - accuracy: 0.1061\n",
      "Epoch 52: val_loss did not improve from 3.21725\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 4.1314 - accuracy: 0.1061 - val_loss: 3.2198 - val_accuracy: 0.2816\n",
      "Epoch 53/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.1127 - accuracy: 0.1107\n",
      "Epoch 53: val_loss improved from 3.21725 to 3.14470, saving model to ./models\\best_model.epoch53-loss3.14.hdf5\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 4.1127 - accuracy: 0.1107 - val_loss: 3.1447 - val_accuracy: 0.2855\n",
      "Epoch 54/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.0906 - accuracy: 0.1076\n",
      "Epoch 54: val_loss did not improve from 3.14470\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 4.0906 - accuracy: 0.1076 - val_loss: 3.1515 - val_accuracy: 0.2790\n",
      "Epoch 55/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.0565 - accuracy: 0.1132\n",
      "Epoch 55: val_loss improved from 3.14470 to 3.13219, saving model to ./models\\best_model.epoch55-loss3.13.hdf5\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 4.0565 - accuracy: 0.1132 - val_loss: 3.1322 - val_accuracy: 0.2849\n",
      "Epoch 56/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.0438 - accuracy: 0.1142\n",
      "Epoch 56: val_loss improved from 3.13219 to 3.12539, saving model to ./models\\best_model.epoch56-loss3.13.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 4.0438 - accuracy: 0.1142 - val_loss: 3.1254 - val_accuracy: 0.2901\n",
      "Epoch 57/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.9894 - accuracy: 0.1230\n",
      "Epoch 57: val_loss improved from 3.12539 to 3.05804, saving model to ./models\\best_model.epoch57-loss3.06.hdf5\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 3.9894 - accuracy: 0.1230 - val_loss: 3.0580 - val_accuracy: 0.2992\n",
      "Epoch 58/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.9494 - accuracy: 0.1307\n",
      "Epoch 58: val_loss did not improve from 3.05804\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 3.9494 - accuracy: 0.1307 - val_loss: 3.0603 - val_accuracy: 0.2966\n",
      "Epoch 59/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.9512 - accuracy: 0.1248\n",
      "Epoch 59: val_loss improved from 3.05804 to 3.03541, saving model to ./models\\best_model.epoch59-loss3.04.hdf5\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 3.9512 - accuracy: 0.1248 - val_loss: 3.0354 - val_accuracy: 0.3057\n",
      "Epoch 60/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.9230 - accuracy: 0.1286\n",
      "Epoch 60: val_loss improved from 3.03541 to 3.03336, saving model to ./models\\best_model.epoch60-loss3.03.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 3.9230 - accuracy: 0.1286 - val_loss: 3.0334 - val_accuracy: 0.3031\n",
      "Epoch 61/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.8802 - accuracy: 0.1343\n",
      "Epoch 61: val_loss improved from 3.03336 to 3.01770, saving model to ./models\\best_model.epoch61-loss3.02.hdf5\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 3.8802 - accuracy: 0.1343 - val_loss: 3.0177 - val_accuracy: 0.3129\n",
      "Epoch 62/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.8944 - accuracy: 0.1323\n",
      "Epoch 62: val_loss improved from 3.01770 to 2.98228, saving model to ./models\\best_model.epoch62-loss2.98.hdf5\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 3.8944 - accuracy: 0.1323 - val_loss: 2.9823 - val_accuracy: 0.3090\n",
      "Epoch 63/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.8630 - accuracy: 0.1337\n",
      "Epoch 63: val_loss did not improve from 2.98228\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 3.8630 - accuracy: 0.1337 - val_loss: 2.9930 - val_accuracy: 0.3155\n",
      "Epoch 64/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.8660 - accuracy: 0.1329\n",
      "Epoch 64: val_loss improved from 2.98228 to 2.94369, saving model to ./models\\best_model.epoch64-loss2.94.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 3.8660 - accuracy: 0.1329 - val_loss: 2.9437 - val_accuracy: 0.3272\n",
      "Epoch 65/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.8029 - accuracy: 0.1494\n",
      "Epoch 65: val_loss improved from 2.94369 to 2.92471, saving model to ./models\\best_model.epoch65-loss2.92.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 3.8029 - accuracy: 0.1494 - val_loss: 2.9247 - val_accuracy: 0.3403\n",
      "Epoch 66/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.7836 - accuracy: 0.1437\n",
      "Epoch 66: val_loss improved from 2.92471 to 2.87626, saving model to ./models\\best_model.epoch66-loss2.88.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 3.7836 - accuracy: 0.1437 - val_loss: 2.8763 - val_accuracy: 0.3299\n",
      "Epoch 67/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.7849 - accuracy: 0.1471\n",
      "Epoch 67: val_loss did not improve from 2.87626\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 3.7849 - accuracy: 0.1471 - val_loss: 2.8782 - val_accuracy: 0.3279\n",
      "Epoch 68/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.7431 - accuracy: 0.1531\n",
      "Epoch 68: val_loss improved from 2.87626 to 2.84800, saving model to ./models\\best_model.epoch68-loss2.85.hdf5\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 3.7431 - accuracy: 0.1531 - val_loss: 2.8480 - val_accuracy: 0.3481\n",
      "Epoch 69/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.7401 - accuracy: 0.1521\n",
      "Epoch 69: val_loss did not improve from 2.84800\n",
      "89/89 [==============================] - 45s 502ms/step - loss: 3.7401 - accuracy: 0.1521 - val_loss: 2.8790 - val_accuracy: 0.3344\n",
      "Epoch 70/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.7133 - accuracy: 0.1541\n",
      "Epoch 70: val_loss improved from 2.84800 to 2.84075, saving model to ./models\\best_model.epoch70-loss2.84.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 3.7133 - accuracy: 0.1541 - val_loss: 2.8407 - val_accuracy: 0.3344\n",
      "Epoch 71/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.6738 - accuracy: 0.1617\n",
      "Epoch 71: val_loss improved from 2.84075 to 2.83508, saving model to ./models\\best_model.epoch71-loss2.84.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 3.6738 - accuracy: 0.1617 - val_loss: 2.8351 - val_accuracy: 0.3435\n",
      "Epoch 72/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.6567 - accuracy: 0.1658\n",
      "Epoch 72: val_loss improved from 2.83508 to 2.79373, saving model to ./models\\best_model.epoch72-loss2.79.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 3.6567 - accuracy: 0.1658 - val_loss: 2.7937 - val_accuracy: 0.3468\n",
      "Epoch 73/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.6421 - accuracy: 0.1641\n",
      "Epoch 73: val_loss improved from 2.79373 to 2.78056, saving model to ./models\\best_model.epoch73-loss2.78.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 3.6421 - accuracy: 0.1641 - val_loss: 2.7806 - val_accuracy: 0.3468\n",
      "Epoch 74/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.6218 - accuracy: 0.1706\n",
      "Epoch 74: val_loss improved from 2.78056 to 2.77891, saving model to ./models\\best_model.epoch74-loss2.78.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 3.6218 - accuracy: 0.1706 - val_loss: 2.7789 - val_accuracy: 0.3690\n",
      "Epoch 75/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.5669 - accuracy: 0.1726\n",
      "Epoch 75: val_loss improved from 2.77891 to 2.74379, saving model to ./models\\best_model.epoch75-loss2.74.hdf5\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 3.5669 - accuracy: 0.1726 - val_loss: 2.7438 - val_accuracy: 0.3605\n",
      "Epoch 76/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.5806 - accuracy: 0.1734\n",
      "Epoch 76: val_loss improved from 2.74379 to 2.72989, saving model to ./models\\best_model.epoch76-loss2.73.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 3.5806 - accuracy: 0.1734 - val_loss: 2.7299 - val_accuracy: 0.3670\n",
      "Epoch 77/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.5718 - accuracy: 0.1710\n",
      "Epoch 77: val_loss did not improve from 2.72989\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 3.5718 - accuracy: 0.1710 - val_loss: 2.7373 - val_accuracy: 0.3579\n",
      "Epoch 78/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.5430 - accuracy: 0.1761\n",
      "Epoch 78: val_loss did not improve from 2.72989\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 3.5430 - accuracy: 0.1761 - val_loss: 2.7395 - val_accuracy: 0.3585\n",
      "Epoch 79/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.4950 - accuracy: 0.1852\n",
      "Epoch 79: val_loss improved from 2.72989 to 2.68840, saving model to ./models\\best_model.epoch79-loss2.69.hdf5\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 3.4950 - accuracy: 0.1852 - val_loss: 2.6884 - val_accuracy: 0.3670\n",
      "Epoch 80/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.4859 - accuracy: 0.1839\n",
      "Epoch 80: val_loss did not improve from 2.68840\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 3.4859 - accuracy: 0.1839 - val_loss: 2.7029 - val_accuracy: 0.3638\n",
      "Epoch 81/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.4889 - accuracy: 0.1836\n",
      "Epoch 81: val_loss improved from 2.68840 to 2.67998, saving model to ./models\\best_model.epoch81-loss2.68.hdf5\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 3.4889 - accuracy: 0.1836 - val_loss: 2.6800 - val_accuracy: 0.3716\n",
      "Epoch 82/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.4485 - accuracy: 0.1952\n",
      "Epoch 82: val_loss improved from 2.67998 to 2.65174, saving model to ./models\\best_model.epoch82-loss2.65.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 3.4485 - accuracy: 0.1952 - val_loss: 2.6517 - val_accuracy: 0.3814\n",
      "Epoch 83/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.4391 - accuracy: 0.1922\n",
      "Epoch 83: val_loss improved from 2.65174 to 2.63002, saving model to ./models\\best_model.epoch83-loss2.63.hdf5\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 3.4391 - accuracy: 0.1922 - val_loss: 2.6300 - val_accuracy: 0.3748\n",
      "Epoch 84/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.3985 - accuracy: 0.2000\n",
      "Epoch 84: val_loss improved from 2.63002 to 2.62244, saving model to ./models\\best_model.epoch84-loss2.62.hdf5\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 3.3985 - accuracy: 0.2000 - val_loss: 2.6224 - val_accuracy: 0.3729\n",
      "Epoch 85/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.3849 - accuracy: 0.1992\n",
      "Epoch 85: val_loss did not improve from 2.62244\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 3.3849 - accuracy: 0.1992 - val_loss: 2.6241 - val_accuracy: 0.3898\n",
      "Epoch 86/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.3584 - accuracy: 0.1977\n",
      "Epoch 86: val_loss improved from 2.62244 to 2.57768, saving model to ./models\\best_model.epoch86-loss2.58.hdf5\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 3.3584 - accuracy: 0.1977 - val_loss: 2.5777 - val_accuracy: 0.3944\n",
      "Epoch 87/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.3361 - accuracy: 0.2107\n",
      "Epoch 87: val_loss did not improve from 2.57768\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 3.3361 - accuracy: 0.2107 - val_loss: 2.6113 - val_accuracy: 0.3827\n",
      "Epoch 88/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.2863 - accuracy: 0.2131\n",
      "Epoch 88: val_loss did not improve from 2.57768\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 3.2863 - accuracy: 0.2131 - val_loss: 2.5798 - val_accuracy: 0.3885\n",
      "Epoch 89/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.3166 - accuracy: 0.2119\n",
      "Epoch 89: val_loss improved from 2.57768 to 2.55061, saving model to ./models\\best_model.epoch89-loss2.55.hdf5\n",
      "89/89 [==============================] - 45s 503ms/step - loss: 3.3166 - accuracy: 0.2119 - val_loss: 2.5506 - val_accuracy: 0.3950\n",
      "Epoch 90/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.2988 - accuracy: 0.2123\n",
      "Epoch 90: val_loss improved from 2.55061 to 2.52538, saving model to ./models\\best_model.epoch90-loss2.53.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 3.2988 - accuracy: 0.2123 - val_loss: 2.5254 - val_accuracy: 0.3931\n",
      "Epoch 91/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.2468 - accuracy: 0.2218\n",
      "Epoch 91: val_loss improved from 2.52538 to 2.50793, saving model to ./models\\best_model.epoch91-loss2.51.hdf5\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 3.2468 - accuracy: 0.2218 - val_loss: 2.5079 - val_accuracy: 0.3990\n",
      "Epoch 92/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.2606 - accuracy: 0.2202\n",
      "Epoch 92: val_loss improved from 2.50793 to 2.50212, saving model to ./models\\best_model.epoch92-loss2.50.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 3.2606 - accuracy: 0.2202 - val_loss: 2.5021 - val_accuracy: 0.4022\n",
      "Epoch 93/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.2301 - accuracy: 0.2232\n",
      "Epoch 93: val_loss improved from 2.50212 to 2.48368, saving model to ./models\\best_model.epoch93-loss2.48.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 3.2301 - accuracy: 0.2232 - val_loss: 2.4837 - val_accuracy: 0.4107\n",
      "Epoch 94/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.1783 - accuracy: 0.2336\n",
      "Epoch 94: val_loss improved from 2.48368 to 2.48307, saving model to ./models\\best_model.epoch94-loss2.48.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 3.1783 - accuracy: 0.2336 - val_loss: 2.4831 - val_accuracy: 0.4094\n",
      "Epoch 95/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.1941 - accuracy: 0.2352\n",
      "Epoch 95: val_loss improved from 2.48307 to 2.47717, saving model to ./models\\best_model.epoch95-loss2.48.hdf5\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 3.1941 - accuracy: 0.2352 - val_loss: 2.4772 - val_accuracy: 0.4166\n",
      "Epoch 96/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.1525 - accuracy: 0.2445\n",
      "Epoch 96: val_loss improved from 2.47717 to 2.46799, saving model to ./models\\best_model.epoch96-loss2.47.hdf5\n",
      "89/89 [==============================] - 44s 502ms/step - loss: 3.1525 - accuracy: 0.2445 - val_loss: 2.4680 - val_accuracy: 0.4126\n",
      "Epoch 97/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.1282 - accuracy: 0.2472\n",
      "Epoch 97: val_loss improved from 2.46799 to 2.44627, saving model to ./models\\best_model.epoch97-loss2.45.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 3.1282 - accuracy: 0.2472 - val_loss: 2.4463 - val_accuracy: 0.4094\n",
      "Epoch 98/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.1694 - accuracy: 0.2342\n",
      "Epoch 98: val_loss improved from 2.44627 to 2.43923, saving model to ./models\\best_model.epoch98-loss2.44.hdf5\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 3.1694 - accuracy: 0.2342 - val_loss: 2.4392 - val_accuracy: 0.4172\n",
      "Epoch 99/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.1344 - accuracy: 0.2447\n",
      "Epoch 99: val_loss improved from 2.43923 to 2.41149, saving model to ./models\\best_model.epoch99-loss2.41.hdf5\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 3.1344 - accuracy: 0.2447 - val_loss: 2.4115 - val_accuracy: 0.4296\n",
      "Epoch 100/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.0957 - accuracy: 0.2517\n",
      "Epoch 100: val_loss did not improve from 2.41149\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 3.0957 - accuracy: 0.2517 - val_loss: 2.4193 - val_accuracy: 0.4172\n",
      "Epoch 101/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.1042 - accuracy: 0.2476\n",
      "Epoch 101: val_loss did not improve from 2.41149\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 3.1042 - accuracy: 0.2476 - val_loss: 2.4394 - val_accuracy: 0.4107\n",
      "Epoch 102/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.0740 - accuracy: 0.2529\n",
      "Epoch 102: val_loss improved from 2.41149 to 2.39748, saving model to ./models\\best_model.epoch102-loss2.40.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 3.0740 - accuracy: 0.2529 - val_loss: 2.3975 - val_accuracy: 0.4185\n",
      "Epoch 103/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.0571 - accuracy: 0.2563\n",
      "Epoch 103: val_loss improved from 2.39748 to 2.37983, saving model to ./models\\best_model.epoch103-loss2.38.hdf5\n",
      "89/89 [==============================] - 45s 503ms/step - loss: 3.0571 - accuracy: 0.2563 - val_loss: 2.3798 - val_accuracy: 0.4270\n",
      "Epoch 104/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.0218 - accuracy: 0.2674\n",
      "Epoch 104: val_loss improved from 2.37983 to 2.35238, saving model to ./models\\best_model.epoch104-loss2.35.hdf5\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 3.0218 - accuracy: 0.2674 - val_loss: 2.3524 - val_accuracy: 0.4289\n",
      "Epoch 105/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.0088 - accuracy: 0.2637\n",
      "Epoch 105: val_loss improved from 2.35238 to 2.34804, saving model to ./models\\best_model.epoch105-loss2.35.hdf5\n",
      "89/89 [==============================] - 44s 492ms/step - loss: 3.0088 - accuracy: 0.2637 - val_loss: 2.3480 - val_accuracy: 0.4322\n",
      "Epoch 106/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.0166 - accuracy: 0.2637\n",
      "Epoch 106: val_loss improved from 2.34804 to 2.33112, saving model to ./models\\best_model.epoch106-loss2.33.hdf5\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 3.0166 - accuracy: 0.2637 - val_loss: 2.3311 - val_accuracy: 0.4335\n",
      "Epoch 107/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.9728 - accuracy: 0.2684\n",
      "Epoch 107: val_loss did not improve from 2.33112\n",
      "89/89 [==============================] - 44s 490ms/step - loss: 2.9728 - accuracy: 0.2684 - val_loss: 2.3319 - val_accuracy: 0.4329\n",
      "Epoch 108/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.9686 - accuracy: 0.2755\n",
      "Epoch 108: val_loss improved from 2.33112 to 2.29972, saving model to ./models\\best_model.epoch108-loss2.30.hdf5\n",
      "89/89 [==============================] - 44s 492ms/step - loss: 2.9686 - accuracy: 0.2755 - val_loss: 2.2997 - val_accuracy: 0.4498\n",
      "Epoch 109/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.9463 - accuracy: 0.2772\n",
      "Epoch 109: val_loss improved from 2.29972 to 2.28667, saving model to ./models\\best_model.epoch109-loss2.29.hdf5\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 2.9463 - accuracy: 0.2772 - val_loss: 2.2867 - val_accuracy: 0.4374\n",
      "Epoch 110/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.9176 - accuracy: 0.2784\n",
      "Epoch 110: val_loss did not improve from 2.28667\n",
      "89/89 [==============================] - 44s 492ms/step - loss: 2.9176 - accuracy: 0.2784 - val_loss: 2.2958 - val_accuracy: 0.4374\n",
      "Epoch 111/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.9254 - accuracy: 0.2769\n",
      "Epoch 111: val_loss did not improve from 2.28667\n",
      "89/89 [==============================] - 44s 491ms/step - loss: 2.9254 - accuracy: 0.2769 - val_loss: 2.2876 - val_accuracy: 0.4302\n",
      "Epoch 112/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.8848 - accuracy: 0.2806\n",
      "Epoch 112: val_loss improved from 2.28667 to 2.28382, saving model to ./models\\best_model.epoch112-loss2.28.hdf5\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 2.8848 - accuracy: 0.2806 - val_loss: 2.2838 - val_accuracy: 0.4316\n",
      "Epoch 113/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.9152 - accuracy: 0.2852\n",
      "Epoch 113: val_loss did not improve from 2.28382\n",
      "89/89 [==============================] - 46s 511ms/step - loss: 2.9152 - accuracy: 0.2852 - val_loss: 2.2860 - val_accuracy: 0.4439\n",
      "Epoch 114/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.8896 - accuracy: 0.2857\n",
      "Epoch 114: val_loss improved from 2.28382 to 2.26223, saving model to ./models\\best_model.epoch114-loss2.26.hdf5\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 2.8896 - accuracy: 0.2857 - val_loss: 2.2622 - val_accuracy: 0.4394\n",
      "Epoch 115/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.8588 - accuracy: 0.2909\n",
      "Epoch 115: val_loss improved from 2.26223 to 2.24946, saving model to ./models\\best_model.epoch115-loss2.25.hdf5\n",
      "89/89 [==============================] - 45s 503ms/step - loss: 2.8588 - accuracy: 0.2909 - val_loss: 2.2495 - val_accuracy: 0.4394\n",
      "Epoch 116/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.8342 - accuracy: 0.2929\n",
      "Epoch 116: val_loss improved from 2.24946 to 2.23485, saving model to ./models\\best_model.epoch116-loss2.23.hdf5\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 2.8342 - accuracy: 0.2929 - val_loss: 2.2349 - val_accuracy: 0.4381\n",
      "Epoch 117/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.8059 - accuracy: 0.2948\n",
      "Epoch 117: val_loss did not improve from 2.23485\n",
      "89/89 [==============================] - 45s 502ms/step - loss: 2.8059 - accuracy: 0.2948 - val_loss: 2.2578 - val_accuracy: 0.4465\n",
      "Epoch 118/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.7987 - accuracy: 0.3039\n",
      "Epoch 118: val_loss did not improve from 2.23485\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 2.7987 - accuracy: 0.3039 - val_loss: 2.2622 - val_accuracy: 0.4465\n",
      "Epoch 119/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.7705 - accuracy: 0.3122\n",
      "Epoch 119: val_loss improved from 2.23485 to 2.20842, saving model to ./models\\best_model.epoch119-loss2.21.hdf5\n",
      "89/89 [==============================] - 45s 503ms/step - loss: 2.7705 - accuracy: 0.3122 - val_loss: 2.2084 - val_accuracy: 0.4511\n",
      "Epoch 120/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.7681 - accuracy: 0.3043\n",
      "Epoch 120: val_loss did not improve from 2.20842\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.7681 - accuracy: 0.3043 - val_loss: 2.2250 - val_accuracy: 0.4557\n",
      "Epoch 121/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.7679 - accuracy: 0.3018\n",
      "Epoch 121: val_loss improved from 2.20842 to 2.19634, saving model to ./models\\best_model.epoch121-loss2.20.hdf5\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 2.7679 - accuracy: 0.3018 - val_loss: 2.1963 - val_accuracy: 0.4544\n",
      "Epoch 122/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.7653 - accuracy: 0.3121\n",
      "Epoch 122: val_loss improved from 2.19634 to 2.17331, saving model to ./models\\best_model.epoch122-loss2.17.hdf5\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 2.7653 - accuracy: 0.3121 - val_loss: 2.1733 - val_accuracy: 0.4550\n",
      "Epoch 123/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.6954 - accuracy: 0.3235\n",
      "Epoch 123: val_loss did not improve from 2.17331\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 2.6954 - accuracy: 0.3235 - val_loss: 2.1823 - val_accuracy: 0.4602\n",
      "Epoch 124/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.6956 - accuracy: 0.3217\n",
      "Epoch 124: val_loss improved from 2.17331 to 2.15880, saving model to ./models\\best_model.epoch124-loss2.16.hdf5\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 2.6956 - accuracy: 0.3217 - val_loss: 2.1588 - val_accuracy: 0.4492\n",
      "Epoch 125/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.6824 - accuracy: 0.3316\n",
      "Epoch 125: val_loss improved from 2.15880 to 2.15459, saving model to ./models\\best_model.epoch125-loss2.15.hdf5\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 2.6824 - accuracy: 0.3316 - val_loss: 2.1546 - val_accuracy: 0.4563\n",
      "Epoch 126/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.6718 - accuracy: 0.3272\n",
      "Epoch 126: val_loss did not improve from 2.15459\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 2.6718 - accuracy: 0.3272 - val_loss: 2.1610 - val_accuracy: 0.4641\n",
      "Epoch 127/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.6517 - accuracy: 0.3334\n",
      "Epoch 127: val_loss improved from 2.15459 to 2.15249, saving model to ./models\\best_model.epoch127-loss2.15.hdf5\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 2.6517 - accuracy: 0.3334 - val_loss: 2.1525 - val_accuracy: 0.4602\n",
      "Epoch 128/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.6327 - accuracy: 0.3389\n",
      "Epoch 128: val_loss improved from 2.15249 to 2.13315, saving model to ./models\\best_model.epoch128-loss2.13.hdf5\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 2.6327 - accuracy: 0.3389 - val_loss: 2.1331 - val_accuracy: 0.4570\n",
      "Epoch 129/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.6434 - accuracy: 0.3343\n",
      "Epoch 129: val_loss did not improve from 2.13315\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.6434 - accuracy: 0.3343 - val_loss: 2.1494 - val_accuracy: 0.4654\n",
      "Epoch 130/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.5882 - accuracy: 0.3385\n",
      "Epoch 130: val_loss improved from 2.13315 to 2.10394, saving model to ./models\\best_model.epoch130-loss2.10.hdf5\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 2.5882 - accuracy: 0.3385 - val_loss: 2.1039 - val_accuracy: 0.4615\n",
      "Epoch 131/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.6194 - accuracy: 0.3446\n",
      "Epoch 131: val_loss improved from 2.10394 to 2.08668, saving model to ./models\\best_model.epoch131-loss2.09.hdf5\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 2.6194 - accuracy: 0.3446 - val_loss: 2.0867 - val_accuracy: 0.4615\n",
      "Epoch 132/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.5635 - accuracy: 0.3508\n",
      "Epoch 132: val_loss did not improve from 2.08668\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 2.5635 - accuracy: 0.3508 - val_loss: 2.0948 - val_accuracy: 0.4668\n",
      "Epoch 133/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.5398 - accuracy: 0.3510\n",
      "Epoch 133: val_loss did not improve from 2.08668\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 2.5398 - accuracy: 0.3510 - val_loss: 2.1022 - val_accuracy: 0.4694\n",
      "Epoch 134/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.5533 - accuracy: 0.3501\n",
      "Epoch 134: val_loss did not improve from 2.08668\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 2.5533 - accuracy: 0.3501 - val_loss: 2.0954 - val_accuracy: 0.4694\n",
      "Epoch 135/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.5321 - accuracy: 0.3502\n",
      "Epoch 135: val_loss improved from 2.08668 to 2.08075, saving model to ./models\\best_model.epoch135-loss2.08.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 2.5321 - accuracy: 0.3502 - val_loss: 2.0808 - val_accuracy: 0.4726\n",
      "Epoch 136/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.5162 - accuracy: 0.3596\n",
      "Epoch 136: val_loss improved from 2.08075 to 2.08067, saving model to ./models\\best_model.epoch136-loss2.08.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.5162 - accuracy: 0.3596 - val_loss: 2.0807 - val_accuracy: 0.4668\n",
      "Epoch 137/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.4782 - accuracy: 0.3678\n",
      "Epoch 137: val_loss improved from 2.08067 to 2.06604, saving model to ./models\\best_model.epoch137-loss2.07.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.4782 - accuracy: 0.3678 - val_loss: 2.0660 - val_accuracy: 0.4694\n",
      "Epoch 138/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.5017 - accuracy: 0.3644\n",
      "Epoch 138: val_loss did not improve from 2.06604\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 2.5017 - accuracy: 0.3644 - val_loss: 2.0706 - val_accuracy: 0.4700\n",
      "Epoch 139/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.4453 - accuracy: 0.3702\n",
      "Epoch 139: val_loss did not improve from 2.06604\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 2.4453 - accuracy: 0.3702 - val_loss: 2.0816 - val_accuracy: 0.4746\n",
      "Epoch 140/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.4683 - accuracy: 0.3727\n",
      "Epoch 140: val_loss improved from 2.06604 to 2.05873, saving model to ./models\\best_model.epoch140-loss2.06.hdf5\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 2.4683 - accuracy: 0.3727 - val_loss: 2.0587 - val_accuracy: 0.4615\n",
      "Epoch 141/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.4576 - accuracy: 0.3768\n",
      "Epoch 141: val_loss improved from 2.05873 to 2.05675, saving model to ./models\\best_model.epoch141-loss2.06.hdf5\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 2.4576 - accuracy: 0.3768 - val_loss: 2.0568 - val_accuracy: 0.4661\n",
      "Epoch 142/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.4450 - accuracy: 0.3726\n",
      "Epoch 142: val_loss improved from 2.05675 to 2.04141, saving model to ./models\\best_model.epoch142-loss2.04.hdf5\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 2.4450 - accuracy: 0.3726 - val_loss: 2.0414 - val_accuracy: 0.4726\n",
      "Epoch 143/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3937 - accuracy: 0.3850\n",
      "Epoch 143: val_loss improved from 2.04141 to 2.03515, saving model to ./models\\best_model.epoch143-loss2.04.hdf5\n",
      "89/89 [==============================] - 45s 504ms/step - loss: 2.3937 - accuracy: 0.3850 - val_loss: 2.0351 - val_accuracy: 0.4733\n",
      "Epoch 144/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3881 - accuracy: 0.3876\n",
      "Epoch 144: val_loss improved from 2.03515 to 2.00356, saving model to ./models\\best_model.epoch144-loss2.00.hdf5\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 2.3881 - accuracy: 0.3876 - val_loss: 2.0036 - val_accuracy: 0.4804\n",
      "Epoch 145/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3756 - accuracy: 0.3881\n",
      "Epoch 145: val_loss did not improve from 2.00356\n",
      "89/89 [==============================] - 45s 502ms/step - loss: 2.3756 - accuracy: 0.3881 - val_loss: 2.0490 - val_accuracy: 0.4726\n",
      "Epoch 146/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3566 - accuracy: 0.3925\n",
      "Epoch 146: val_loss improved from 2.00356 to 2.00326, saving model to ./models\\best_model.epoch146-loss2.00.hdf5\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 2.3566 - accuracy: 0.3925 - val_loss: 2.0033 - val_accuracy: 0.4791\n",
      "Epoch 147/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3657 - accuracy: 0.3897\n",
      "Epoch 147: val_loss did not improve from 2.00326\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 2.3657 - accuracy: 0.3897 - val_loss: 2.0044 - val_accuracy: 0.4720\n",
      "Epoch 148/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3608 - accuracy: 0.3891\n",
      "Epoch 148: val_loss did not improve from 2.00326\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 2.3608 - accuracy: 0.3891 - val_loss: 2.0137 - val_accuracy: 0.4733\n",
      "Epoch 149/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3062 - accuracy: 0.4123\n",
      "Epoch 149: val_loss improved from 2.00326 to 1.99218, saving model to ./models\\best_model.epoch149-loss1.99.hdf5\n",
      "89/89 [==============================] - 45s 505ms/step - loss: 2.3062 - accuracy: 0.4123 - val_loss: 1.9922 - val_accuracy: 0.4837\n",
      "Epoch 150/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3191 - accuracy: 0.4051\n",
      "Epoch 150: val_loss improved from 1.99218 to 1.96042, saving model to ./models\\best_model.epoch150-loss1.96.hdf5\n",
      "89/89 [==============================] - 45s 504ms/step - loss: 2.3191 - accuracy: 0.4051 - val_loss: 1.9604 - val_accuracy: 0.4831\n",
      "Epoch 151/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3207 - accuracy: 0.4004\n",
      "Epoch 151: val_loss improved from 1.96042 to 1.96013, saving model to ./models\\best_model.epoch151-loss1.96.hdf5\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 2.3207 - accuracy: 0.4004 - val_loss: 1.9601 - val_accuracy: 0.4824\n",
      "Epoch 152/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2842 - accuracy: 0.4085\n",
      "Epoch 152: val_loss did not improve from 1.96013\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 2.2842 - accuracy: 0.4085 - val_loss: 1.9729 - val_accuracy: 0.4798\n",
      "Epoch 153/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2702 - accuracy: 0.4095\n",
      "Epoch 153: val_loss did not improve from 1.96013\n",
      "89/89 [==============================] - 45s 502ms/step - loss: 2.2702 - accuracy: 0.4095 - val_loss: 1.9932 - val_accuracy: 0.4804\n",
      "Epoch 154/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2480 - accuracy: 0.4219\n",
      "Epoch 154: val_loss did not improve from 1.96013\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.2480 - accuracy: 0.4219 - val_loss: 1.9611 - val_accuracy: 0.4824\n",
      "Epoch 155/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2611 - accuracy: 0.4055\n",
      "Epoch 155: val_loss did not improve from 1.96013\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 2.2611 - accuracy: 0.4055 - val_loss: 1.9614 - val_accuracy: 0.4798\n",
      "Epoch 156/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2161 - accuracy: 0.4144\n",
      "Epoch 156: val_loss did not improve from 1.96013\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 2.2161 - accuracy: 0.4144 - val_loss: 1.9751 - val_accuracy: 0.4804\n",
      "Epoch 157/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2091 - accuracy: 0.4338\n",
      "Epoch 157: val_loss improved from 1.96013 to 1.95909, saving model to ./models\\best_model.epoch157-loss1.96.hdf5\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 2.2091 - accuracy: 0.4338 - val_loss: 1.9591 - val_accuracy: 0.4909\n",
      "Epoch 158/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2076 - accuracy: 0.4177\n",
      "Epoch 158: val_loss improved from 1.95909 to 1.92868, saving model to ./models\\best_model.epoch158-loss1.93.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.2076 - accuracy: 0.4177 - val_loss: 1.9287 - val_accuracy: 0.4928\n",
      "Epoch 159/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1875 - accuracy: 0.4358\n",
      "Epoch 159: val_loss did not improve from 1.92868\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.1875 - accuracy: 0.4358 - val_loss: 1.9416 - val_accuracy: 0.4850\n",
      "Epoch 160/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1916 - accuracy: 0.4291\n",
      "Epoch 160: val_loss did not improve from 1.92868\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 2.1916 - accuracy: 0.4291 - val_loss: 1.9373 - val_accuracy: 0.4922\n",
      "Epoch 161/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1725 - accuracy: 0.4281\n",
      "Epoch 161: val_loss did not improve from 1.92868\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 2.1725 - accuracy: 0.4281 - val_loss: 1.9315 - val_accuracy: 0.4870\n",
      "Epoch 162/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1502 - accuracy: 0.4433\n",
      "Epoch 162: val_loss did not improve from 1.92868\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 2.1502 - accuracy: 0.4433 - val_loss: 1.9363 - val_accuracy: 0.4915\n",
      "Epoch 163/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1276 - accuracy: 0.4464\n",
      "Epoch 163: val_loss improved from 1.92868 to 1.91224, saving model to ./models\\best_model.epoch163-loss1.91.hdf5\n",
      "89/89 [==============================] - 45s 505ms/step - loss: 2.1276 - accuracy: 0.4464 - val_loss: 1.9122 - val_accuracy: 0.4915\n",
      "Epoch 164/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1090 - accuracy: 0.4417\n",
      "Epoch 164: val_loss did not improve from 1.91224\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.1090 - accuracy: 0.4417 - val_loss: 1.9208 - val_accuracy: 0.4876\n",
      "Epoch 165/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1076 - accuracy: 0.4537\n",
      "Epoch 165: val_loss improved from 1.91224 to 1.90532, saving model to ./models\\best_model.epoch165-loss1.91.hdf5\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 2.1076 - accuracy: 0.4537 - val_loss: 1.9053 - val_accuracy: 0.4941\n",
      "Epoch 166/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1110 - accuracy: 0.4442\n",
      "Epoch 166: val_loss did not improve from 1.90532\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.1110 - accuracy: 0.4442 - val_loss: 1.9134 - val_accuracy: 0.4765\n",
      "Epoch 167/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0920 - accuracy: 0.4461\n",
      "Epoch 167: val_loss did not improve from 1.90532\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.0920 - accuracy: 0.4461 - val_loss: 1.9187 - val_accuracy: 0.4909\n",
      "Epoch 168/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0608 - accuracy: 0.4585\n",
      "Epoch 168: val_loss improved from 1.90532 to 1.88719, saving model to ./models\\best_model.epoch168-loss1.89.hdf5\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 2.0608 - accuracy: 0.4585 - val_loss: 1.8872 - val_accuracy: 0.4883\n",
      "Epoch 169/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0454 - accuracy: 0.4678\n",
      "Epoch 169: val_loss improved from 1.88719 to 1.87006, saving model to ./models\\best_model.epoch169-loss1.87.hdf5\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 2.0454 - accuracy: 0.4678 - val_loss: 1.8701 - val_accuracy: 0.4954\n",
      "Epoch 170/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0669 - accuracy: 0.4557\n",
      "Epoch 170: val_loss improved from 1.87006 to 1.86917, saving model to ./models\\best_model.epoch170-loss1.87.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 2.0669 - accuracy: 0.4557 - val_loss: 1.8692 - val_accuracy: 0.4974\n",
      "Epoch 171/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0493 - accuracy: 0.4637\n",
      "Epoch 171: val_loss improved from 1.86917 to 1.86164, saving model to ./models\\best_model.epoch171-loss1.86.hdf5\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 2.0493 - accuracy: 0.4637 - val_loss: 1.8616 - val_accuracy: 0.4980\n",
      "Epoch 172/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0068 - accuracy: 0.4711\n",
      "Epoch 172: val_loss did not improve from 1.86164\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 2.0068 - accuracy: 0.4711 - val_loss: 1.8658 - val_accuracy: 0.4954\n",
      "Epoch 173/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0080 - accuracy: 0.4718\n",
      "Epoch 173: val_loss did not improve from 1.86164\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 2.0080 - accuracy: 0.4718 - val_loss: 1.8799 - val_accuracy: 0.4993\n",
      "Epoch 174/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0076 - accuracy: 0.4684\n",
      "Epoch 174: val_loss did not improve from 1.86164\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 2.0076 - accuracy: 0.4684 - val_loss: 1.8661 - val_accuracy: 0.5000\n",
      "Epoch 175/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.9835 - accuracy: 0.4705\n",
      "Epoch 175: val_loss did not improve from 1.86164\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 1.9835 - accuracy: 0.4705 - val_loss: 1.8621 - val_accuracy: 0.4967\n",
      "Epoch 176/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.9698 - accuracy: 0.4773\n",
      "Epoch 176: val_loss did not improve from 1.86164\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 1.9698 - accuracy: 0.4773 - val_loss: 1.8628 - val_accuracy: 0.4928\n",
      "Epoch 177/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.9456 - accuracy: 0.4872\n",
      "Epoch 177: val_loss improved from 1.86164 to 1.84682, saving model to ./models\\best_model.epoch177-loss1.85.hdf5\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 1.9456 - accuracy: 0.4872 - val_loss: 1.8468 - val_accuracy: 0.5007\n",
      "Epoch 178/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.9555 - accuracy: 0.4828\n",
      "Epoch 178: val_loss improved from 1.84682 to 1.83552, saving model to ./models\\best_model.epoch178-loss1.84.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 1.9555 - accuracy: 0.4828 - val_loss: 1.8355 - val_accuracy: 0.4993\n",
      "Epoch 179/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.9143 - accuracy: 0.4955\n",
      "Epoch 179: val_loss did not improve from 1.83552\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 1.9143 - accuracy: 0.4955 - val_loss: 1.8544 - val_accuracy: 0.5039\n",
      "Epoch 180/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.9191 - accuracy: 0.4906\n",
      "Epoch 180: val_loss improved from 1.83552 to 1.82871, saving model to ./models\\best_model.epoch180-loss1.83.hdf5\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 1.9191 - accuracy: 0.4906 - val_loss: 1.8287 - val_accuracy: 0.5098\n",
      "Epoch 181/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.8970 - accuracy: 0.4953\n",
      "Epoch 181: val_loss improved from 1.82871 to 1.82071, saving model to ./models\\best_model.epoch181-loss1.82.hdf5\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 1.8970 - accuracy: 0.4953 - val_loss: 1.8207 - val_accuracy: 0.5098\n",
      "Epoch 182/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.9212 - accuracy: 0.4859\n",
      "Epoch 182: val_loss improved from 1.82071 to 1.81933, saving model to ./models\\best_model.epoch182-loss1.82.hdf5\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 1.9212 - accuracy: 0.4859 - val_loss: 1.8193 - val_accuracy: 0.5143\n",
      "Epoch 183/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.8948 - accuracy: 0.4974\n",
      "Epoch 183: val_loss did not improve from 1.81933\n",
      "89/89 [==============================] - 45s 498ms/step - loss: 1.8948 - accuracy: 0.4974 - val_loss: 1.8297 - val_accuracy: 0.5130\n",
      "Epoch 184/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.8544 - accuracy: 0.5053\n",
      "Epoch 184: val_loss did not improve from 1.81933\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 1.8544 - accuracy: 0.5053 - val_loss: 1.8209 - val_accuracy: 0.5013\n",
      "Epoch 185/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.8532 - accuracy: 0.5057\n",
      "Epoch 185: val_loss did not improve from 1.81933\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 1.8532 - accuracy: 0.5057 - val_loss: 1.8318 - val_accuracy: 0.4987\n",
      "Epoch 186/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.8225 - accuracy: 0.5131\n",
      "Epoch 186: val_loss did not improve from 1.81933\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 1.8225 - accuracy: 0.5131 - val_loss: 1.8194 - val_accuracy: 0.5026\n",
      "Epoch 187/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.8580 - accuracy: 0.5100\n",
      "Epoch 187: val_loss improved from 1.81933 to 1.81330, saving model to ./models\\best_model.epoch187-loss1.81.hdf5\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 1.8580 - accuracy: 0.5100 - val_loss: 1.8133 - val_accuracy: 0.5072\n",
      "Epoch 188/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.8366 - accuracy: 0.5141\n",
      "Epoch 188: val_loss did not improve from 1.81330\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 1.8366 - accuracy: 0.5141 - val_loss: 1.8230 - val_accuracy: 0.5052\n",
      "Epoch 189/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.8410 - accuracy: 0.5056\n",
      "Epoch 189: val_loss improved from 1.81330 to 1.80730, saving model to ./models\\best_model.epoch189-loss1.81.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 1.8410 - accuracy: 0.5056 - val_loss: 1.8073 - val_accuracy: 0.5111\n",
      "Epoch 190/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.7930 - accuracy: 0.5206\n",
      "Epoch 190: val_loss improved from 1.80730 to 1.79626, saving model to ./models\\best_model.epoch190-loss1.80.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 1.7930 - accuracy: 0.5206 - val_loss: 1.7963 - val_accuracy: 0.5098\n",
      "Epoch 191/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.7946 - accuracy: 0.5230\n",
      "Epoch 191: val_loss did not improve from 1.79626\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 1.7946 - accuracy: 0.5230 - val_loss: 1.8082 - val_accuracy: 0.5124\n",
      "Epoch 192/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.7801 - accuracy: 0.5316\n",
      "Epoch 192: val_loss did not improve from 1.79626\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 1.7801 - accuracy: 0.5316 - val_loss: 1.7992 - val_accuracy: 0.5124\n",
      "Epoch 193/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.7519 - accuracy: 0.5319\n",
      "Epoch 193: val_loss improved from 1.79626 to 1.79099, saving model to ./models\\best_model.epoch193-loss1.79.hdf5\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 1.7519 - accuracy: 0.5319 - val_loss: 1.7910 - val_accuracy: 0.5124\n",
      "Epoch 194/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.7347 - accuracy: 0.5442\n",
      "Epoch 194: val_loss did not improve from 1.79099\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 1.7347 - accuracy: 0.5442 - val_loss: 1.7975 - val_accuracy: 0.5169\n",
      "Epoch 195/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.7659 - accuracy: 0.5273\n",
      "Epoch 195: val_loss did not improve from 1.79099\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 1.7659 - accuracy: 0.5273 - val_loss: 1.8075 - val_accuracy: 0.5020\n",
      "Epoch 196/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.7309 - accuracy: 0.5306\n",
      "Epoch 196: val_loss did not improve from 1.79099\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 1.7309 - accuracy: 0.5306 - val_loss: 1.7941 - val_accuracy: 0.5065\n",
      "Epoch 197/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.7155 - accuracy: 0.5381\n",
      "Epoch 197: val_loss did not improve from 1.79099\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 1.7155 - accuracy: 0.5381 - val_loss: 1.7922 - val_accuracy: 0.5117\n",
      "Epoch 198/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.7047 - accuracy: 0.5377\n",
      "Epoch 198: val_loss improved from 1.79099 to 1.77070, saving model to ./models\\best_model.epoch198-loss1.77.hdf5\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 1.7047 - accuracy: 0.5377 - val_loss: 1.7707 - val_accuracy: 0.5163\n",
      "Epoch 199/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.6955 - accuracy: 0.5487\n",
      "Epoch 199: val_loss did not improve from 1.77070\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 1.6955 - accuracy: 0.5487 - val_loss: 1.8075 - val_accuracy: 0.5026\n",
      "Epoch 200/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.6979 - accuracy: 0.5486\n",
      "Epoch 200: val_loss did not improve from 1.77070\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 1.6979 - accuracy: 0.5486 - val_loss: 1.7778 - val_accuracy: 0.5189\n",
      "Epoch 201/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.6675 - accuracy: 0.5570\n",
      "Epoch 201: val_loss did not improve from 1.77070\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 1.6675 - accuracy: 0.5570 - val_loss: 1.7769 - val_accuracy: 0.5085\n",
      "Epoch 202/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.6513 - accuracy: 0.5584\n",
      "Epoch 202: val_loss did not improve from 1.77070\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 1.6513 - accuracy: 0.5584 - val_loss: 1.7792 - val_accuracy: 0.5111\n",
      "Epoch 203/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.6507 - accuracy: 0.5566\n",
      "Epoch 203: val_loss did not improve from 1.77070\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 1.6507 - accuracy: 0.5566 - val_loss: 1.7821 - val_accuracy: 0.5150\n",
      "Epoch 204/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.6457 - accuracy: 0.5607\n",
      "Epoch 204: val_loss improved from 1.77070 to 1.77000, saving model to ./models\\best_model.epoch204-loss1.77.hdf5\n",
      "89/89 [==============================] - 46s 510ms/step - loss: 1.6457 - accuracy: 0.5607 - val_loss: 1.7700 - val_accuracy: 0.5150\n",
      "Epoch 205/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.6295 - accuracy: 0.5683\n",
      "Epoch 205: val_loss did not improve from 1.77000\n",
      "89/89 [==============================] - 46s 522ms/step - loss: 1.6295 - accuracy: 0.5683 - val_loss: 1.7939 - val_accuracy: 0.5091\n",
      "Epoch 206/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.6311 - accuracy: 0.5662\n",
      "Epoch 206: val_loss improved from 1.77000 to 1.76253, saving model to ./models\\best_model.epoch206-loss1.76.hdf5\n",
      "89/89 [==============================] - 46s 512ms/step - loss: 1.6311 - accuracy: 0.5662 - val_loss: 1.7625 - val_accuracy: 0.5156\n",
      "Epoch 207/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.6068 - accuracy: 0.5645\n",
      "Epoch 207: val_loss did not improve from 1.76253\n",
      "89/89 [==============================] - 46s 511ms/step - loss: 1.6068 - accuracy: 0.5645 - val_loss: 1.7736 - val_accuracy: 0.5228\n",
      "Epoch 208/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.5854 - accuracy: 0.5690\n",
      "Epoch 208: val_loss did not improve from 1.76253\n",
      "89/89 [==============================] - 43s 487ms/step - loss: 1.5854 - accuracy: 0.5690 - val_loss: 1.7631 - val_accuracy: 0.5098\n",
      "Epoch 209/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.5629 - accuracy: 0.5867\n",
      "Epoch 209: val_loss did not improve from 1.76253\n",
      "89/89 [==============================] - 45s 506ms/step - loss: 1.5629 - accuracy: 0.5867 - val_loss: 1.7719 - val_accuracy: 0.5091\n",
      "Epoch 210/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.5717 - accuracy: 0.5801\n",
      "Epoch 210: val_loss improved from 1.76253 to 1.76011, saving model to ./models\\best_model.epoch210-loss1.76.hdf5\n",
      "89/89 [==============================] - 45s 504ms/step - loss: 1.5717 - accuracy: 0.5801 - val_loss: 1.7601 - val_accuracy: 0.5143\n",
      "Epoch 211/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.5589 - accuracy: 0.5867\n",
      "Epoch 211: val_loss improved from 1.76011 to 1.73803, saving model to ./models\\best_model.epoch211-loss1.74.hdf5\n",
      "89/89 [==============================] - 45s 509ms/step - loss: 1.5589 - accuracy: 0.5867 - val_loss: 1.7380 - val_accuracy: 0.5209\n",
      "Epoch 212/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.5370 - accuracy: 0.5917\n",
      "Epoch 212: val_loss did not improve from 1.73803\n",
      "89/89 [==============================] - 45s 508ms/step - loss: 1.5370 - accuracy: 0.5917 - val_loss: 1.7723 - val_accuracy: 0.5196\n",
      "Epoch 213/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.5392 - accuracy: 0.5897\n",
      "Epoch 213: val_loss did not improve from 1.73803\n",
      "89/89 [==============================] - 46s 515ms/step - loss: 1.5392 - accuracy: 0.5897 - val_loss: 1.7419 - val_accuracy: 0.5196\n",
      "Epoch 214/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.5289 - accuracy: 0.5890\n",
      "Epoch 214: val_loss did not improve from 1.73803\n",
      "89/89 [==============================] - 45s 506ms/step - loss: 1.5289 - accuracy: 0.5890 - val_loss: 1.7428 - val_accuracy: 0.5274\n",
      "Epoch 215/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.5094 - accuracy: 0.5955\n",
      "Epoch 215: val_loss improved from 1.73803 to 1.72863, saving model to ./models\\best_model.epoch215-loss1.73.hdf5\n",
      "89/89 [==============================] - 46s 511ms/step - loss: 1.5094 - accuracy: 0.5955 - val_loss: 1.7286 - val_accuracy: 0.5359\n",
      "Epoch 216/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.4963 - accuracy: 0.5979\n",
      "Epoch 216: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 45s 509ms/step - loss: 1.4963 - accuracy: 0.5979 - val_loss: 1.7540 - val_accuracy: 0.5104\n",
      "Epoch 217/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.4665 - accuracy: 0.6109\n",
      "Epoch 217: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 45s 508ms/step - loss: 1.4665 - accuracy: 0.6109 - val_loss: 1.7557 - val_accuracy: 0.5228\n",
      "Epoch 218/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.4468 - accuracy: 0.6124\n",
      "Epoch 218: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 45s 504ms/step - loss: 1.4468 - accuracy: 0.6124 - val_loss: 1.7447 - val_accuracy: 0.5202\n",
      "Epoch 219/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.4587 - accuracy: 0.6103\n",
      "Epoch 219: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 45s 506ms/step - loss: 1.4587 - accuracy: 0.6103 - val_loss: 1.7546 - val_accuracy: 0.5137\n",
      "Epoch 220/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.4559 - accuracy: 0.6103\n",
      "Epoch 220: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 46s 511ms/step - loss: 1.4559 - accuracy: 0.6103 - val_loss: 1.7561 - val_accuracy: 0.5059\n",
      "Epoch 221/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.4481 - accuracy: 0.6045\n",
      "Epoch 221: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 46s 512ms/step - loss: 1.4481 - accuracy: 0.6045 - val_loss: 1.7486 - val_accuracy: 0.5150\n",
      "Epoch 222/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.4440 - accuracy: 0.6107\n",
      "Epoch 222: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 46s 513ms/step - loss: 1.4440 - accuracy: 0.6107 - val_loss: 1.7340 - val_accuracy: 0.5189\n",
      "Epoch 223/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.4050 - accuracy: 0.6242\n",
      "Epoch 223: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 46s 510ms/step - loss: 1.4050 - accuracy: 0.6242 - val_loss: 1.7344 - val_accuracy: 0.5280\n",
      "Epoch 224/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.4229 - accuracy: 0.6201\n",
      "Epoch 224: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 45s 509ms/step - loss: 1.4229 - accuracy: 0.6201 - val_loss: 1.7366 - val_accuracy: 0.5176\n",
      "Epoch 225/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.4231 - accuracy: 0.6150\n",
      "Epoch 225: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 46s 511ms/step - loss: 1.4231 - accuracy: 0.6150 - val_loss: 1.7522 - val_accuracy: 0.5156\n",
      "Epoch 226/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.4115 - accuracy: 0.6253\n",
      "Epoch 226: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 45s 504ms/step - loss: 1.4115 - accuracy: 0.6253 - val_loss: 1.7433 - val_accuracy: 0.5215\n",
      "Epoch 227/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.3650 - accuracy: 0.6301\n",
      "Epoch 227: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 45s 509ms/step - loss: 1.3650 - accuracy: 0.6301 - val_loss: 1.7287 - val_accuracy: 0.5228\n",
      "Epoch 228/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.3819 - accuracy: 0.6302\n",
      "Epoch 228: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 46s 512ms/step - loss: 1.3819 - accuracy: 0.6302 - val_loss: 1.7336 - val_accuracy: 0.5306\n",
      "Epoch 229/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.3717 - accuracy: 0.6315\n",
      "Epoch 229: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 46s 519ms/step - loss: 1.3717 - accuracy: 0.6315 - val_loss: 1.7349 - val_accuracy: 0.5248\n",
      "Epoch 230/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.3628 - accuracy: 0.6331\n",
      "Epoch 230: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 45s 509ms/step - loss: 1.3628 - accuracy: 0.6331 - val_loss: 1.7347 - val_accuracy: 0.5306\n",
      "Epoch 231/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.3405 - accuracy: 0.6462\n",
      "Epoch 231: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 46s 513ms/step - loss: 1.3405 - accuracy: 0.6462 - val_loss: 1.7300 - val_accuracy: 0.5280\n",
      "Epoch 232/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.3405 - accuracy: 0.6332\n",
      "Epoch 232: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 45s 506ms/step - loss: 1.3405 - accuracy: 0.6332 - val_loss: 1.7324 - val_accuracy: 0.5169\n",
      "Epoch 233/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.3230 - accuracy: 0.6428\n",
      "Epoch 233: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 46s 513ms/step - loss: 1.3230 - accuracy: 0.6428 - val_loss: 1.7295 - val_accuracy: 0.5150\n",
      "Epoch 234/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.3049 - accuracy: 0.6469\n",
      "Epoch 234: val_loss did not improve from 1.72863\n",
      "89/89 [==============================] - 45s 507ms/step - loss: 1.3049 - accuracy: 0.6469 - val_loss: 1.7492 - val_accuracy: 0.5222\n",
      "Epoch 235/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.3064 - accuracy: 0.6459\n",
      "Epoch 235: val_loss improved from 1.72863 to 1.72040, saving model to ./models\\best_model.epoch235-loss1.72.hdf5\n",
      "89/89 [==============================] - 46s 516ms/step - loss: 1.3064 - accuracy: 0.6459 - val_loss: 1.7204 - val_accuracy: 0.5241\n",
      "Epoch 236/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.2910 - accuracy: 0.6530\n",
      "Epoch 236: val_loss did not improve from 1.72040\n",
      "89/89 [==============================] - 46s 519ms/step - loss: 1.2910 - accuracy: 0.6530 - val_loss: 1.7339 - val_accuracy: 0.5215\n",
      "Epoch 237/500\n",
      "36/89 [===========>..................] - ETA: 26s - loss: 1.2786 - accuracy: 0.6688"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m80\u001b[39m  \u001b[39m# Choose a batch size that fits your memory constraints\u001b[39;00m\n\u001b[0;32m      2\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[1;32m----> 4\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(datagen\u001b[39m.\u001b[39;49mflow(train_images, train_labels, batch_size\u001b[39m=\u001b[39;49mbatch_size),\n\u001b[0;32m      5\u001b[0m                     steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(train_images) \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m batch_size,\n\u001b[0;32m      6\u001b[0m                     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m      7\u001b[0m                     validation_data\u001b[39m=\u001b[39;49m(val_images, val_labels),\n\u001b[0;32m      8\u001b[0m                     callbacks\u001b[39m=\u001b[39;49m[callbacks])\n\u001b[0;32m      9\u001b[0m \u001b[39m#)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 80  # Choose a batch size that fits your memory constraints\n",
    "epochs = 500\n",
    "\n",
    "history = model.fit(datagen.flow(train_images, train_labels, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(train_images) // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    callbacks=[callbacks])\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 - 5s - loss: 1.7111 - accuracy: 0.5538 - 5s/epoch - 110ms/step\n",
      "\n",
      "Test accuracy: 0.553816020488739\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('pre_trained_38.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
