{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id                     breed\n",
      "0      000bec180eb18c7604dcecc8fe0dba07               boston_bull\n",
      "1      001513dfcb2ffafc82cccf4d8bbaba97                     dingo\n",
      "2      001cdf01b096e06d78e9e5112d419397                  pekinese\n",
      "3      00214f311d5d2247d5dfe4fe24b2303d                  bluetick\n",
      "4      0021f9ceb3235effd7fcde7f7538ed62          golden_retriever\n",
      "...                                 ...                       ...\n",
      "10217  ffd25009d635cfd16e793503ac5edef0                    borzoi\n",
      "10218  ffd3f636f7f379c51ba3648a9ff8254f            dandie_dinmont\n",
      "10219  ffe2ca6c940cddfee68fa3cc6c63213f                  airedale\n",
      "10220  ffe5f6d8e2bff356e9482a80a6e29aac        miniature_pinscher\n",
      "10221  fff43b07992508bc822f33d8ffd902ae  chesapeake_bay_retriever\n",
      "\n",
      "[10222 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#TEST_IMAGE_DIR = \"./dog-breed-identification/test/\"\n",
    "TRAIN_IMAGE_DIR = \"./dog-breed-identification/train/\"\n",
    "LABELS = './dog-breed-identification/labels.csv'\n",
    "\n",
    "data = pd.read_csv(LABELS)\n",
    "class_names = data['breed'].unique()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data from the csv only contans the ids of the photos and not the photos itself\n",
    "#Iterate through the data and check whether the photo is in test or train\n",
    "#Then fetch it and store it in its proper variable\n",
    "import tensorflow as tf\n",
    "from os.path import join\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def load_and_preprocess_images(image_dir, image_ids, target_size=(224, 224)):\n",
    "    image_data = []\n",
    "    for img_id in image_ids:\n",
    "        img_path = join(image_dir, img_id + \".jpg\")\n",
    "        img = load_img(img_path, target_size=target_size)\n",
    "        img_array = img_to_array(img)\n",
    "        preprocessed_img = preprocess_input(img_array) #I dont flatten the image here\n",
    "        image_data.append(preprocessed_img)\n",
    "    return np.array(image_data)\n",
    "\n",
    "# Get image IDs for train and test\n",
    "train_image_ids = data['id'].values\n",
    "labels = data['breed'].values\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "train_image_ids, unfinished_test_image_ids, train_labels, unfinished_test_labels = train_test_split(train_image_ids, encoded_labels, test_size=0.3, random_state=42, stratify=encoded_labels)\n",
    "\n",
    "test_image_ids, val_image_ids, test_labels, val_labels = train_test_split(unfinished_test_image_ids, unfinished_test_labels, test_size=0.5, random_state=42, stratify=unfinished_test_labels)\n",
    "\n",
    "train_images = load_and_preprocess_images(TRAIN_IMAGE_DIR, train_image_ids)\n",
    "test_images = load_and_preprocess_images(TRAIN_IMAGE_DIR, test_image_ids)\n",
    "val_images = load_and_preprocess_images(TRAIN_IMAGE_DIR, val_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 250.0\n",
    "test_images = test_images / 250.0\n",
    "val_images = val_images / 250.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2\n",
    ")\n",
    "\n",
    "datagen.fit(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Set the layers in the base model to non-trainable\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build the custom classification head\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "predictions = tf.keras.layers.Dense(120, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#from keras.callbacks import Callback\n",
    "\n",
    "filepath = './models/best_model.epoch{epoch:02d}-loss{val_loss:.2f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, \n",
    "                             monitor='val_loss',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.3431 - accuracy: 0.0235\n",
      "Epoch 1: val_loss improved from inf to 4.53881, saving model to ./models\\best_model.epoch01-loss4.54.hdf5\n",
      "89/89 [==============================] - 58s 580ms/step - loss: 5.3431 - accuracy: 0.0235 - val_loss: 4.5388 - val_accuracy: 0.0600\n",
      "Epoch 2/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 4.4388 - accuracy: 0.0717\n",
      "Epoch 2: val_loss improved from 4.53881 to 4.17532, saving model to ./models\\best_model.epoch02-loss4.18.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 4.4388 - accuracy: 0.0717 - val_loss: 4.1753 - val_accuracy: 0.1219\n",
      "Epoch 3/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.9673 - accuracy: 0.1149\n",
      "Epoch 3: val_loss improved from 4.17532 to 3.65549, saving model to ./models\\best_model.epoch03-loss3.66.hdf5\n",
      "89/89 [==============================] - 45s 507ms/step - loss: 3.9673 - accuracy: 0.1149 - val_loss: 3.6555 - val_accuracy: 0.2190\n",
      "Epoch 4/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.5915 - accuracy: 0.1559\n",
      "Epoch 4: val_loss improved from 3.65549 to 3.16132, saving model to ./models\\best_model.epoch04-loss3.16.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 3.5915 - accuracy: 0.1559 - val_loss: 3.1613 - val_accuracy: 0.2640\n",
      "Epoch 5/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.3746 - accuracy: 0.1874\n",
      "Epoch 5: val_loss improved from 3.16132 to 2.86922, saving model to ./models\\best_model.epoch05-loss2.87.hdf5\n",
      "89/89 [==============================] - 45s 509ms/step - loss: 3.3746 - accuracy: 0.1874 - val_loss: 2.8692 - val_accuracy: 0.3096\n",
      "Epoch 6/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.2315 - accuracy: 0.2085\n",
      "Epoch 6: val_loss improved from 2.86922 to 2.72459, saving model to ./models\\best_model.epoch06-loss2.72.hdf5\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 3.2315 - accuracy: 0.2085 - val_loss: 2.7246 - val_accuracy: 0.3096\n",
      "Epoch 7/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 3.0756 - accuracy: 0.2293\n",
      "Epoch 7: val_loss improved from 2.72459 to 2.60198, saving model to ./models\\best_model.epoch07-loss2.60.hdf5\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 3.0756 - accuracy: 0.2293 - val_loss: 2.6020 - val_accuracy: 0.3325\n",
      "Epoch 8/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.9996 - accuracy: 0.2506\n",
      "Epoch 8: val_loss did not improve from 2.60198\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 2.9996 - accuracy: 0.2506 - val_loss: 2.6098 - val_accuracy: 0.3194\n",
      "Epoch 9/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.9127 - accuracy: 0.2694\n",
      "Epoch 9: val_loss did not improve from 2.60198\n",
      "89/89 [==============================] - 46s 510ms/step - loss: 2.9127 - accuracy: 0.2694 - val_loss: 2.6611 - val_accuracy: 0.3162\n",
      "Epoch 10/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.8506 - accuracy: 0.2753\n",
      "Epoch 10: val_loss improved from 2.60198 to 2.47802, saving model to ./models\\best_model.epoch10-loss2.48.hdf5\n",
      "89/89 [==============================] - 46s 510ms/step - loss: 2.8506 - accuracy: 0.2753 - val_loss: 2.4780 - val_accuracy: 0.3670\n",
      "Epoch 11/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.8247 - accuracy: 0.2854\n",
      "Epoch 11: val_loss did not improve from 2.47802\n",
      "89/89 [==============================] - 45s 506ms/step - loss: 2.8247 - accuracy: 0.2854 - val_loss: 2.4884 - val_accuracy: 0.3462\n",
      "Epoch 12/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.7248 - accuracy: 0.2968\n",
      "Epoch 12: val_loss improved from 2.47802 to 2.42063, saving model to ./models\\best_model.epoch12-loss2.42.hdf5\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 2.7248 - accuracy: 0.2968 - val_loss: 2.4206 - val_accuracy: 0.3735\n",
      "Epoch 13/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.7214 - accuracy: 0.3028\n",
      "Epoch 13: val_loss improved from 2.42063 to 2.40272, saving model to ./models\\best_model.epoch13-loss2.40.hdf5\n",
      "89/89 [==============================] - 45s 510ms/step - loss: 2.7214 - accuracy: 0.3028 - val_loss: 2.4027 - val_accuracy: 0.3761\n",
      "Epoch 14/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.6844 - accuracy: 0.3149\n",
      "Epoch 14: val_loss did not improve from 2.40272\n",
      "89/89 [==============================] - 45s 504ms/step - loss: 2.6844 - accuracy: 0.3149 - val_loss: 2.4405 - val_accuracy: 0.3683\n",
      "Epoch 15/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.6178 - accuracy: 0.3227\n",
      "Epoch 15: val_loss did not improve from 2.40272\n",
      "89/89 [==============================] - 45s 504ms/step - loss: 2.6178 - accuracy: 0.3227 - val_loss: 2.5109 - val_accuracy: 0.3520\n",
      "Epoch 16/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.6193 - accuracy: 0.3257\n",
      "Epoch 16: val_loss did not improve from 2.40272\n",
      "89/89 [==============================] - 45s 505ms/step - loss: 2.6193 - accuracy: 0.3257 - val_loss: 2.4451 - val_accuracy: 0.3664\n",
      "Epoch 17/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.5718 - accuracy: 0.3285\n",
      "Epoch 17: val_loss did not improve from 2.40272\n",
      "89/89 [==============================] - 45s 505ms/step - loss: 2.5718 - accuracy: 0.3285 - val_loss: 2.4582 - val_accuracy: 0.3696\n",
      "Epoch 18/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.5876 - accuracy: 0.3302\n",
      "Epoch 18: val_loss improved from 2.40272 to 2.36996, saving model to ./models\\best_model.epoch18-loss2.37.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.5876 - accuracy: 0.3302 - val_loss: 2.3700 - val_accuracy: 0.3781\n",
      "Epoch 19/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.5702 - accuracy: 0.3322\n",
      "Epoch 19: val_loss did not improve from 2.36996\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.5702 - accuracy: 0.3322 - val_loss: 2.3999 - val_accuracy: 0.3722\n",
      "Epoch 20/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.5307 - accuracy: 0.3440\n",
      "Epoch 20: val_loss improved from 2.36996 to 2.34118, saving model to ./models\\best_model.epoch20-loss2.34.hdf5\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 2.5307 - accuracy: 0.3440 - val_loss: 2.3412 - val_accuracy: 0.3866\n",
      "Epoch 21/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.4980 - accuracy: 0.3500\n",
      "Epoch 21: val_loss did not improve from 2.34118\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 2.4980 - accuracy: 0.3500 - val_loss: 2.3461 - val_accuracy: 0.3879\n",
      "Epoch 22/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.4956 - accuracy: 0.3556\n",
      "Epoch 22: val_loss improved from 2.34118 to 2.33351, saving model to ./models\\best_model.epoch22-loss2.33.hdf5\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 2.4956 - accuracy: 0.3556 - val_loss: 2.3335 - val_accuracy: 0.3879\n",
      "Epoch 23/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.4619 - accuracy: 0.3583\n",
      "Epoch 23: val_loss did not improve from 2.33351\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.4619 - accuracy: 0.3583 - val_loss: 2.3357 - val_accuracy: 0.3950\n",
      "Epoch 24/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.4594 - accuracy: 0.3566\n",
      "Epoch 24: val_loss improved from 2.33351 to 2.26061, saving model to ./models\\best_model.epoch24-loss2.26.hdf5\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 2.4594 - accuracy: 0.3566 - val_loss: 2.2606 - val_accuracy: 0.4140\n",
      "Epoch 25/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.4493 - accuracy: 0.3586\n",
      "Epoch 25: val_loss did not improve from 2.26061\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 2.4493 - accuracy: 0.3586 - val_loss: 2.4086 - val_accuracy: 0.3833\n",
      "Epoch 26/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.4312 - accuracy: 0.3624\n",
      "Epoch 26: val_loss did not improve from 2.26061\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 2.4312 - accuracy: 0.3624 - val_loss: 2.4640 - val_accuracy: 0.3625\n",
      "Epoch 27/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.4041 - accuracy: 0.3614\n",
      "Epoch 27: val_loss did not improve from 2.26061\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 2.4041 - accuracy: 0.3614 - val_loss: 2.3382 - val_accuracy: 0.3950\n",
      "Epoch 28/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.4118 - accuracy: 0.3726\n",
      "Epoch 28: val_loss did not improve from 2.26061\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.4118 - accuracy: 0.3726 - val_loss: 2.3002 - val_accuracy: 0.4166\n",
      "Epoch 29/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3760 - accuracy: 0.3709\n",
      "Epoch 29: val_loss did not improve from 2.26061\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 2.3760 - accuracy: 0.3709 - val_loss: 2.2909 - val_accuracy: 0.3866\n",
      "Epoch 30/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3856 - accuracy: 0.3733\n",
      "Epoch 30: val_loss did not improve from 2.26061\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.3856 - accuracy: 0.3733 - val_loss: 2.2809 - val_accuracy: 0.3977\n",
      "Epoch 31/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3944 - accuracy: 0.3719\n",
      "Epoch 31: val_loss did not improve from 2.26061\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 2.3944 - accuracy: 0.3719 - val_loss: 2.3296 - val_accuracy: 0.3957\n",
      "Epoch 32/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3543 - accuracy: 0.3784\n",
      "Epoch 32: val_loss did not improve from 2.26061\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 2.3543 - accuracy: 0.3784 - val_loss: 2.2951 - val_accuracy: 0.3970\n",
      "Epoch 33/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3715 - accuracy: 0.3702\n",
      "Epoch 33: val_loss did not improve from 2.26061\n",
      "89/89 [==============================] - 45s 502ms/step - loss: 2.3715 - accuracy: 0.3702 - val_loss: 2.2651 - val_accuracy: 0.3918\n",
      "Epoch 34/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3428 - accuracy: 0.3812\n",
      "Epoch 34: val_loss did not improve from 2.26061\n",
      "89/89 [==============================] - 45s 505ms/step - loss: 2.3428 - accuracy: 0.3812 - val_loss: 2.2895 - val_accuracy: 0.4016\n",
      "Epoch 35/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3600 - accuracy: 0.3815\n",
      "Epoch 35: val_loss did not improve from 2.26061\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 2.3600 - accuracy: 0.3815 - val_loss: 2.3832 - val_accuracy: 0.3768\n",
      "Epoch 36/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3238 - accuracy: 0.3818\n",
      "Epoch 36: val_loss did not improve from 2.26061\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.3238 - accuracy: 0.3818 - val_loss: 2.3559 - val_accuracy: 0.3768\n",
      "Epoch 37/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3220 - accuracy: 0.3871\n",
      "Epoch 37: val_loss improved from 2.26061 to 2.22730, saving model to ./models\\best_model.epoch37-loss2.23.hdf5\n",
      "89/89 [==============================] - 45s 508ms/step - loss: 2.3220 - accuracy: 0.3871 - val_loss: 2.2273 - val_accuracy: 0.4094\n",
      "Epoch 38/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3053 - accuracy: 0.3873\n",
      "Epoch 38: val_loss did not improve from 2.22730\n",
      "89/89 [==============================] - 46s 513ms/step - loss: 2.3053 - accuracy: 0.3873 - val_loss: 2.2748 - val_accuracy: 0.3983\n",
      "Epoch 39/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3198 - accuracy: 0.3856\n",
      "Epoch 39: val_loss did not improve from 2.22730\n",
      "89/89 [==============================] - 45s 503ms/step - loss: 2.3198 - accuracy: 0.3856 - val_loss: 2.2840 - val_accuracy: 0.4022\n",
      "Epoch 40/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3314 - accuracy: 0.3763\n",
      "Epoch 40: val_loss did not improve from 2.22730\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 2.3314 - accuracy: 0.3763 - val_loss: 2.2552 - val_accuracy: 0.4094\n",
      "Epoch 41/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2946 - accuracy: 0.3976\n",
      "Epoch 41: val_loss did not improve from 2.22730\n",
      "89/89 [==============================] - 45s 502ms/step - loss: 2.2946 - accuracy: 0.3976 - val_loss: 2.2476 - val_accuracy: 0.4159\n",
      "Epoch 42/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.3135 - accuracy: 0.3918\n",
      "Epoch 42: val_loss did not improve from 2.22730\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.3135 - accuracy: 0.3918 - val_loss: 2.3367 - val_accuracy: 0.3892\n",
      "Epoch 43/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2859 - accuracy: 0.3921\n",
      "Epoch 43: val_loss did not improve from 2.22730\n",
      "89/89 [==============================] - 45s 502ms/step - loss: 2.2859 - accuracy: 0.3921 - val_loss: 2.2282 - val_accuracy: 0.4126\n",
      "Epoch 44/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2839 - accuracy: 0.3888\n",
      "Epoch 44: val_loss did not improve from 2.22730\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.2839 - accuracy: 0.3888 - val_loss: 2.3194 - val_accuracy: 0.3977\n",
      "Epoch 45/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2818 - accuracy: 0.3960\n",
      "Epoch 45: val_loss did not improve from 2.22730\n",
      "89/89 [==============================] - 45s 502ms/step - loss: 2.2818 - accuracy: 0.3960 - val_loss: 2.3009 - val_accuracy: 0.4068\n",
      "Epoch 46/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2899 - accuracy: 0.3976\n",
      "Epoch 46: val_loss improved from 2.22730 to 2.22449, saving model to ./models\\best_model.epoch46-loss2.22.hdf5\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 2.2899 - accuracy: 0.3976 - val_loss: 2.2245 - val_accuracy: 0.4146\n",
      "Epoch 47/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2639 - accuracy: 0.3934\n",
      "Epoch 47: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 45s 503ms/step - loss: 2.2639 - accuracy: 0.3934 - val_loss: 2.3639 - val_accuracy: 0.3840\n",
      "Epoch 48/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2730 - accuracy: 0.3935\n",
      "Epoch 48: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 45s 509ms/step - loss: 2.2730 - accuracy: 0.3935 - val_loss: 2.2318 - val_accuracy: 0.4107\n",
      "Epoch 49/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2661 - accuracy: 0.3962\n",
      "Epoch 49: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 45s 505ms/step - loss: 2.2661 - accuracy: 0.3962 - val_loss: 2.3014 - val_accuracy: 0.3977\n",
      "Epoch 50/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2496 - accuracy: 0.4045\n",
      "Epoch 50: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 2.2496 - accuracy: 0.4045 - val_loss: 2.3164 - val_accuracy: 0.3918\n",
      "Epoch 51/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2675 - accuracy: 0.3993\n",
      "Epoch 51: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 45s 501ms/step - loss: 2.2675 - accuracy: 0.3993 - val_loss: 2.2344 - val_accuracy: 0.3963\n",
      "Epoch 52/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2629 - accuracy: 0.3996\n",
      "Epoch 52: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 2.2629 - accuracy: 0.3996 - val_loss: 2.2495 - val_accuracy: 0.3996\n",
      "Epoch 53/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2464 - accuracy: 0.4099\n",
      "Epoch 53: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 45s 508ms/step - loss: 2.2464 - accuracy: 0.4099 - val_loss: 2.3187 - val_accuracy: 0.3892\n",
      "Epoch 54/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2333 - accuracy: 0.4048\n",
      "Epoch 54: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 2.2333 - accuracy: 0.4048 - val_loss: 2.3461 - val_accuracy: 0.3924\n",
      "Epoch 55/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2567 - accuracy: 0.4048\n",
      "Epoch 55: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 45s 502ms/step - loss: 2.2567 - accuracy: 0.4048 - val_loss: 2.2894 - val_accuracy: 0.3996\n",
      "Epoch 56/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2415 - accuracy: 0.4028\n",
      "Epoch 56: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 45s 499ms/step - loss: 2.2415 - accuracy: 0.4028 - val_loss: 2.2677 - val_accuracy: 0.4126\n",
      "Epoch 57/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2354 - accuracy: 0.4037\n",
      "Epoch 57: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 45s 506ms/step - loss: 2.2354 - accuracy: 0.4037 - val_loss: 2.3947 - val_accuracy: 0.3977\n",
      "Epoch 58/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2337 - accuracy: 0.4088\n",
      "Epoch 58: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 2.2337 - accuracy: 0.4088 - val_loss: 2.3366 - val_accuracy: 0.3983\n",
      "Epoch 59/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2376 - accuracy: 0.3977\n",
      "Epoch 59: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 2.2376 - accuracy: 0.3977 - val_loss: 2.3604 - val_accuracy: 0.3944\n",
      "Epoch 60/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2441 - accuracy: 0.3952\n",
      "Epoch 60: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 490ms/step - loss: 2.2441 - accuracy: 0.3952 - val_loss: 2.2984 - val_accuracy: 0.3990\n",
      "Epoch 61/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2171 - accuracy: 0.4035\n",
      "Epoch 61: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 2.2171 - accuracy: 0.4035 - val_loss: 2.2607 - val_accuracy: 0.4055\n",
      "Epoch 62/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2436 - accuracy: 0.4010\n",
      "Epoch 62: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 491ms/step - loss: 2.2436 - accuracy: 0.4010 - val_loss: 2.3585 - val_accuracy: 0.4016\n",
      "Epoch 63/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2092 - accuracy: 0.4133\n",
      "Epoch 63: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.2092 - accuracy: 0.4133 - val_loss: 2.3326 - val_accuracy: 0.4016\n",
      "Epoch 64/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1943 - accuracy: 0.4062\n",
      "Epoch 64: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 490ms/step - loss: 2.1943 - accuracy: 0.4062 - val_loss: 2.3667 - val_accuracy: 0.3892\n",
      "Epoch 65/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2321 - accuracy: 0.4102\n",
      "Epoch 65: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.2321 - accuracy: 0.4102 - val_loss: 2.3060 - val_accuracy: 0.3924\n",
      "Epoch 66/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2003 - accuracy: 0.4085\n",
      "Epoch 66: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 490ms/step - loss: 2.2003 - accuracy: 0.4085 - val_loss: 2.2510 - val_accuracy: 0.4113\n",
      "Epoch 67/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2165 - accuracy: 0.4061\n",
      "Epoch 67: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.2165 - accuracy: 0.4061 - val_loss: 2.2761 - val_accuracy: 0.4048\n",
      "Epoch 68/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1991 - accuracy: 0.4109\n",
      "Epoch 68: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 491ms/step - loss: 2.1991 - accuracy: 0.4109 - val_loss: 2.2892 - val_accuracy: 0.4022\n",
      "Epoch 69/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2042 - accuracy: 0.4109\n",
      "Epoch 69: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 2.2042 - accuracy: 0.4109 - val_loss: 2.2478 - val_accuracy: 0.4179\n",
      "Epoch 70/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2315 - accuracy: 0.4064\n",
      "Epoch 70: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 490ms/step - loss: 2.2315 - accuracy: 0.4064 - val_loss: 2.4589 - val_accuracy: 0.3846\n",
      "Epoch 71/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2074 - accuracy: 0.4096\n",
      "Epoch 71: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.2074 - accuracy: 0.4096 - val_loss: 2.2976 - val_accuracy: 0.4146\n",
      "Epoch 72/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2122 - accuracy: 0.4095\n",
      "Epoch 72: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 490ms/step - loss: 2.2122 - accuracy: 0.4095 - val_loss: 2.2460 - val_accuracy: 0.4231\n",
      "Epoch 73/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1719 - accuracy: 0.4155\n",
      "Epoch 73: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 2.1719 - accuracy: 0.4155 - val_loss: 2.2832 - val_accuracy: 0.4068\n",
      "Epoch 74/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2046 - accuracy: 0.4113\n",
      "Epoch 74: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.2046 - accuracy: 0.4113 - val_loss: 2.2511 - val_accuracy: 0.4211\n",
      "Epoch 75/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1885 - accuracy: 0.4157\n",
      "Epoch 75: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 492ms/step - loss: 2.1885 - accuracy: 0.4157 - val_loss: 2.2934 - val_accuracy: 0.4074\n",
      "Epoch 76/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1796 - accuracy: 0.4109\n",
      "Epoch 76: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 2.1796 - accuracy: 0.4109 - val_loss: 2.2497 - val_accuracy: 0.4061\n",
      "Epoch 77/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1632 - accuracy: 0.4223\n",
      "Epoch 77: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 2.1632 - accuracy: 0.4223 - val_loss: 2.2373 - val_accuracy: 0.4250\n",
      "Epoch 78/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1882 - accuracy: 0.4174\n",
      "Epoch 78: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 489ms/step - loss: 2.1882 - accuracy: 0.4174 - val_loss: 2.2330 - val_accuracy: 0.4218\n",
      "Epoch 79/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1837 - accuracy: 0.4116\n",
      "Epoch 79: val_loss did not improve from 2.22449\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 2.1837 - accuracy: 0.4116 - val_loss: 2.2628 - val_accuracy: 0.4081\n",
      "Epoch 80/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.2017 - accuracy: 0.4092\n",
      "Epoch 80: val_loss improved from 2.22449 to 2.21924, saving model to ./models\\best_model.epoch80-loss2.22.hdf5\n",
      "89/89 [==============================] - 44s 491ms/step - loss: 2.2017 - accuracy: 0.4092 - val_loss: 2.2192 - val_accuracy: 0.4022\n",
      "Epoch 81/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1797 - accuracy: 0.4157\n",
      "Epoch 81: val_loss did not improve from 2.21924\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 2.1797 - accuracy: 0.4157 - val_loss: 2.2760 - val_accuracy: 0.4035\n",
      "Epoch 82/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1787 - accuracy: 0.4123\n",
      "Epoch 82: val_loss did not improve from 2.21924\n",
      "89/89 [==============================] - 44s 490ms/step - loss: 2.1787 - accuracy: 0.4123 - val_loss: 2.2693 - val_accuracy: 0.4133\n",
      "Epoch 83/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1844 - accuracy: 0.4137\n",
      "Epoch 83: val_loss did not improve from 2.21924\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.1844 - accuracy: 0.4137 - val_loss: 2.2202 - val_accuracy: 0.4166\n",
      "Epoch 84/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1789 - accuracy: 0.4132\n",
      "Epoch 84: val_loss did not improve from 2.21924\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.1789 - accuracy: 0.4132 - val_loss: 2.2650 - val_accuracy: 0.4205\n",
      "Epoch 85/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1598 - accuracy: 0.4250\n",
      "Epoch 85: val_loss did not improve from 2.21924\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 2.1598 - accuracy: 0.4250 - val_loss: 2.2720 - val_accuracy: 0.4166\n",
      "Epoch 86/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1954 - accuracy: 0.4161\n",
      "Epoch 86: val_loss did not improve from 2.21924\n",
      "89/89 [==============================] - 44s 490ms/step - loss: 2.1954 - accuracy: 0.4161 - val_loss: 2.3071 - val_accuracy: 0.4068\n",
      "Epoch 87/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1755 - accuracy: 0.4148\n",
      "Epoch 87: val_loss did not improve from 2.21924\n",
      "89/89 [==============================] - 44s 491ms/step - loss: 2.1755 - accuracy: 0.4148 - val_loss: 2.2701 - val_accuracy: 0.4224\n",
      "Epoch 88/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1740 - accuracy: 0.4131\n",
      "Epoch 88: val_loss did not improve from 2.21924\n",
      "89/89 [==============================] - 44s 491ms/step - loss: 2.1740 - accuracy: 0.4131 - val_loss: 2.2776 - val_accuracy: 0.4237\n",
      "Epoch 89/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1629 - accuracy: 0.4245\n",
      "Epoch 89: val_loss did not improve from 2.21924\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.1629 - accuracy: 0.4245 - val_loss: 2.2833 - val_accuracy: 0.4153\n",
      "Epoch 90/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1715 - accuracy: 0.4167\n",
      "Epoch 90: val_loss improved from 2.21924 to 2.19320, saving model to ./models\\best_model.epoch90-loss2.19.hdf5\n",
      "89/89 [==============================] - 44s 499ms/step - loss: 2.1715 - accuracy: 0.4167 - val_loss: 2.1932 - val_accuracy: 0.4289\n",
      "Epoch 91/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1478 - accuracy: 0.4185\n",
      "Epoch 91: val_loss improved from 2.19320 to 2.18277, saving model to ./models\\best_model.epoch91-loss2.18.hdf5\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.1478 - accuracy: 0.4185 - val_loss: 2.1828 - val_accuracy: 0.4426\n",
      "Epoch 92/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1636 - accuracy: 0.4229\n",
      "Epoch 92: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 491ms/step - loss: 2.1636 - accuracy: 0.4229 - val_loss: 2.2452 - val_accuracy: 0.4309\n",
      "Epoch 93/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1519 - accuracy: 0.4177\n",
      "Epoch 93: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 492ms/step - loss: 2.1519 - accuracy: 0.4177 - val_loss: 2.2135 - val_accuracy: 0.4218\n",
      "Epoch 94/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1213 - accuracy: 0.4260\n",
      "Epoch 94: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.1213 - accuracy: 0.4260 - val_loss: 2.2258 - val_accuracy: 0.4244\n",
      "Epoch 95/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1439 - accuracy: 0.4201\n",
      "Epoch 95: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 2.1439 - accuracy: 0.4201 - val_loss: 2.2014 - val_accuracy: 0.4250\n",
      "Epoch 96/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1371 - accuracy: 0.4324\n",
      "Epoch 96: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 492ms/step - loss: 2.1371 - accuracy: 0.4324 - val_loss: 2.2982 - val_accuracy: 0.4074\n",
      "Epoch 97/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1731 - accuracy: 0.4175\n",
      "Epoch 97: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 2.1731 - accuracy: 0.4175 - val_loss: 2.3297 - val_accuracy: 0.4087\n",
      "Epoch 98/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1413 - accuracy: 0.4264\n",
      "Epoch 98: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 492ms/step - loss: 2.1413 - accuracy: 0.4264 - val_loss: 2.3404 - val_accuracy: 0.3937\n",
      "Epoch 99/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1322 - accuracy: 0.4249\n",
      "Epoch 99: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.1322 - accuracy: 0.4249 - val_loss: 2.5187 - val_accuracy: 0.3833\n",
      "Epoch 100/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1468 - accuracy: 0.4257\n",
      "Epoch 100: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.1468 - accuracy: 0.4257 - val_loss: 2.2508 - val_accuracy: 0.4166\n",
      "Epoch 101/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1529 - accuracy: 0.4232\n",
      "Epoch 101: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 2.1529 - accuracy: 0.4232 - val_loss: 2.3283 - val_accuracy: 0.4042\n",
      "Epoch 102/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1633 - accuracy: 0.4208\n",
      "Epoch 102: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.1633 - accuracy: 0.4208 - val_loss: 2.2435 - val_accuracy: 0.4237\n",
      "Epoch 103/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1868 - accuracy: 0.4116\n",
      "Epoch 103: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 45s 508ms/step - loss: 2.1868 - accuracy: 0.4116 - val_loss: 2.2182 - val_accuracy: 0.4250\n",
      "Epoch 104/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1496 - accuracy: 0.4264\n",
      "Epoch 104: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 45s 505ms/step - loss: 2.1496 - accuracy: 0.4264 - val_loss: 2.2241 - val_accuracy: 0.4316\n",
      "Epoch 105/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1287 - accuracy: 0.4139\n",
      "Epoch 105: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 46s 515ms/step - loss: 2.1287 - accuracy: 0.4139 - val_loss: 2.2642 - val_accuracy: 0.4153\n",
      "Epoch 106/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1172 - accuracy: 0.4328\n",
      "Epoch 106: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 45s 506ms/step - loss: 2.1172 - accuracy: 0.4328 - val_loss: 2.2598 - val_accuracy: 0.4120\n",
      "Epoch 107/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1325 - accuracy: 0.4304\n",
      "Epoch 107: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 46s 518ms/step - loss: 2.1325 - accuracy: 0.4304 - val_loss: 2.2157 - val_accuracy: 0.4276\n",
      "Epoch 108/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1109 - accuracy: 0.4342\n",
      "Epoch 108: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 45s 509ms/step - loss: 2.1109 - accuracy: 0.4342 - val_loss: 2.2950 - val_accuracy: 0.4159\n",
      "Epoch 109/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1050 - accuracy: 0.4310\n",
      "Epoch 109: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 2.1050 - accuracy: 0.4310 - val_loss: 2.2106 - val_accuracy: 0.4309\n",
      "Epoch 110/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1069 - accuracy: 0.4297\n",
      "Epoch 110: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.1069 - accuracy: 0.4297 - val_loss: 2.2466 - val_accuracy: 0.4179\n",
      "Epoch 111/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1561 - accuracy: 0.4208\n",
      "Epoch 111: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 45s 504ms/step - loss: 2.1561 - accuracy: 0.4208 - val_loss: 2.3157 - val_accuracy: 0.4035\n",
      "Epoch 112/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1105 - accuracy: 0.4281\n",
      "Epoch 112: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 492ms/step - loss: 2.1105 - accuracy: 0.4281 - val_loss: 2.2546 - val_accuracy: 0.4205\n",
      "Epoch 113/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1008 - accuracy: 0.4297\n",
      "Epoch 113: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.1008 - accuracy: 0.4297 - val_loss: 2.2127 - val_accuracy: 0.4244\n",
      "Epoch 114/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1531 - accuracy: 0.4225\n",
      "Epoch 114: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 2.1531 - accuracy: 0.4225 - val_loss: 2.2265 - val_accuracy: 0.4133\n",
      "Epoch 115/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0881 - accuracy: 0.4336\n",
      "Epoch 115: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 2.0881 - accuracy: 0.4336 - val_loss: 2.2243 - val_accuracy: 0.4231\n",
      "Epoch 116/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1354 - accuracy: 0.4222\n",
      "Epoch 116: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 492ms/step - loss: 2.1354 - accuracy: 0.4222 - val_loss: 2.2000 - val_accuracy: 0.4322\n",
      "Epoch 117/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1257 - accuracy: 0.4339\n",
      "Epoch 117: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.1257 - accuracy: 0.4339 - val_loss: 2.2576 - val_accuracy: 0.4224\n",
      "Epoch 118/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1174 - accuracy: 0.4274\n",
      "Epoch 118: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.1174 - accuracy: 0.4274 - val_loss: 2.2705 - val_accuracy: 0.4113\n",
      "Epoch 119/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0818 - accuracy: 0.4359\n",
      "Epoch 119: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 496ms/step - loss: 2.0818 - accuracy: 0.4359 - val_loss: 2.2653 - val_accuracy: 0.4185\n",
      "Epoch 120/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1027 - accuracy: 0.4291\n",
      "Epoch 120: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 492ms/step - loss: 2.1027 - accuracy: 0.4291 - val_loss: 2.2717 - val_accuracy: 0.4120\n",
      "Epoch 121/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1153 - accuracy: 0.4325\n",
      "Epoch 121: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.1153 - accuracy: 0.4325 - val_loss: 2.2688 - val_accuracy: 0.4061\n",
      "Epoch 122/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0976 - accuracy: 0.4327\n",
      "Epoch 122: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.0976 - accuracy: 0.4327 - val_loss: 2.2846 - val_accuracy: 0.4244\n",
      "Epoch 123/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1245 - accuracy: 0.4334\n",
      "Epoch 123: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.1245 - accuracy: 0.4334 - val_loss: 2.1986 - val_accuracy: 0.4413\n",
      "Epoch 124/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1472 - accuracy: 0.4301\n",
      "Epoch 124: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 492ms/step - loss: 2.1472 - accuracy: 0.4301 - val_loss: 2.2348 - val_accuracy: 0.4218\n",
      "Epoch 125/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0966 - accuracy: 0.4355\n",
      "Epoch 125: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.0966 - accuracy: 0.4355 - val_loss: 2.2114 - val_accuracy: 0.4257\n",
      "Epoch 126/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0849 - accuracy: 0.4399\n",
      "Epoch 126: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.0849 - accuracy: 0.4399 - val_loss: 2.2814 - val_accuracy: 0.4133\n",
      "Epoch 127/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1048 - accuracy: 0.4311\n",
      "Epoch 127: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.1048 - accuracy: 0.4311 - val_loss: 2.2477 - val_accuracy: 0.4146\n",
      "Epoch 128/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1066 - accuracy: 0.4302\n",
      "Epoch 128: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 2.1066 - accuracy: 0.4302 - val_loss: 2.3080 - val_accuracy: 0.4094\n",
      "Epoch 129/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1037 - accuracy: 0.4342\n",
      "Epoch 129: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 2.1037 - accuracy: 0.4342 - val_loss: 2.2639 - val_accuracy: 0.4042\n",
      "Epoch 130/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1164 - accuracy: 0.4294\n",
      "Epoch 130: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.1164 - accuracy: 0.4294 - val_loss: 2.2202 - val_accuracy: 0.4309\n",
      "Epoch 131/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0851 - accuracy: 0.4414\n",
      "Epoch 131: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 497ms/step - loss: 2.0851 - accuracy: 0.4414 - val_loss: 2.2193 - val_accuracy: 0.4302\n",
      "Epoch 132/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0802 - accuracy: 0.4392\n",
      "Epoch 132: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.0802 - accuracy: 0.4392 - val_loss: 2.2426 - val_accuracy: 0.4218\n",
      "Epoch 133/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0888 - accuracy: 0.4363\n",
      "Epoch 133: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 2.0888 - accuracy: 0.4363 - val_loss: 2.2055 - val_accuracy: 0.4276\n",
      "Epoch 134/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0924 - accuracy: 0.4351\n",
      "Epoch 134: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.0924 - accuracy: 0.4351 - val_loss: 2.2192 - val_accuracy: 0.4342\n",
      "Epoch 135/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0843 - accuracy: 0.4375\n",
      "Epoch 135: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 2.0843 - accuracy: 0.4375 - val_loss: 2.2240 - val_accuracy: 0.4218\n",
      "Epoch 136/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0830 - accuracy: 0.4387\n",
      "Epoch 136: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 2.0830 - accuracy: 0.4387 - val_loss: 2.2603 - val_accuracy: 0.4113\n",
      "Epoch 137/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.1007 - accuracy: 0.4387\n",
      "Epoch 137: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 45s 504ms/step - loss: 2.1007 - accuracy: 0.4387 - val_loss: 2.2752 - val_accuracy: 0.4120\n",
      "Epoch 138/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0977 - accuracy: 0.4396\n",
      "Epoch 138: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.0977 - accuracy: 0.4396 - val_loss: 2.2671 - val_accuracy: 0.4042\n",
      "Epoch 139/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0990 - accuracy: 0.4298\n",
      "Epoch 139: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 494ms/step - loss: 2.0990 - accuracy: 0.4298 - val_loss: 2.3096 - val_accuracy: 0.3963\n",
      "Epoch 140/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0813 - accuracy: 0.4367\n",
      "Epoch 140: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.0813 - accuracy: 0.4367 - val_loss: 2.2496 - val_accuracy: 0.4107\n",
      "Epoch 141/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0645 - accuracy: 0.4353\n",
      "Epoch 141: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 45s 500ms/step - loss: 2.0645 - accuracy: 0.4353 - val_loss: 2.2637 - val_accuracy: 0.4257\n",
      "Epoch 142/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0614 - accuracy: 0.4362\n",
      "Epoch 142: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.0614 - accuracy: 0.4362 - val_loss: 2.2549 - val_accuracy: 0.4179\n",
      "Epoch 143/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0756 - accuracy: 0.4401\n",
      "Epoch 143: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 2.0756 - accuracy: 0.4401 - val_loss: 2.2613 - val_accuracy: 0.4166\n",
      "Epoch 144/500\n",
      "89/89 [==============================] - ETA: 0s - loss: 2.0710 - accuracy: 0.4475\n",
      "Epoch 144: val_loss did not improve from 2.18277\n",
      "89/89 [==============================] - 44s 493ms/step - loss: 2.0710 - accuracy: 0.4475 - val_loss: 2.2925 - val_accuracy: 0.4029\n",
      "Epoch 145/500\n",
      "66/89 [=====================>........] - ETA: 10s - loss: 2.0795 - accuracy: 0.4346"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m80\u001b[39m  \u001b[39m# Choose a batch size that fits your memory constraints\u001b[39;00m\n\u001b[0;32m      2\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[1;32m----> 4\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(datagen\u001b[39m.\u001b[39;49mflow(train_images, train_labels, batch_size\u001b[39m=\u001b[39;49mbatch_size),\n\u001b[0;32m      5\u001b[0m                     steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(train_images) \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m batch_size,\n\u001b[0;32m      6\u001b[0m                     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m      7\u001b[0m                     validation_data\u001b[39m=\u001b[39;49m(val_images, val_labels),\n\u001b[0;32m      8\u001b[0m                     callbacks\u001b[39m=\u001b[39;49m[callbacks])\n\u001b[0;32m      9\u001b[0m \u001b[39m#)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Danny\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 80  # Choose a batch size that fits your memory constraints\n",
    "epochs = 500\n",
    "\n",
    "history = model.fit(datagen.flow(train_images, train_labels, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(train_images) // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    callbacks=[callbacks])\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "#print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('pre_trained_38.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
